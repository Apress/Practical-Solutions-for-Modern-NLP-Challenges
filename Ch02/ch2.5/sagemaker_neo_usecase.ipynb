{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical Support Ticket Triage using SageMaker Neo and an LLM Model\n",
    "\n",
    "This notebook demonstrates how to compile a large language model (LLM) using Amazon SageMaker Neo and deploy the compiled model on-device. The use case addresses the problem of slow manual categorization of technical support tickets by automating the process. The model is designed to classify tickets into categories such as **Billing** and **Technical**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We begin by importing the necessary libraries and setting up environment variables. Make sure to update the S3 bucket name and other variables as needed for your AWS environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import os\n",
    "\n",
    "# Set the AWS region\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# Get the SageMaker execution role\n",
    "role = get_execution_role()\n",
    "\n",
    "# Specify your S3 bucket name (update this value)\n",
    "bucket = 'your-s3-bucket-name'  # Replace with your S3 bucket name\n",
    "\n",
    "# Define S3 paths for the pre-trained model artifact and for compiled output\n",
    "model_data = f's3://{bucket}/models/ticket_classifier.tar.gz'  # Pre-trained model artifact\n",
    "compiled_output_path = f's3://{bucket}/compiled-models/'\n",
    "\n",
    "print(f'AWS Region: {region}')\n",
    "print(f'Model Data: {model_data}')\n",
    "print(f'Compiled Model Output Path: {compiled_output_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Compilation with SageMaker Neo\n",
    "\n",
    "In this section, we define a SageMaker model and compile it with SageMaker Neo. The compilation step optimizes the model for the target device family (for example, a CPU instance type like `ml_c5`).\n",
    "\n",
    "For a text classification model, the input shape might represent token IDs with a fixed sequence length. Adjust the `input_shape` parameter as required by your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "\n",
    "# Create a SageMaker Model object using the pre-trained model artifact\n",
    "model = Model(\n",
    "    model_data=model_data,\n",
    "    role=role,\n",
    "    framework_version='1.9.0',  # Example framework version; adjust as needed\n",
    "    sagemaker_session=sagemaker.Session()\n",
    ")\n",
    "\n",
    "# Compile the model for the target instance family. The input shape here is an example for a text model.\n",
    "compiled_model = model.compile(\n",
    "    target_instance_family='ml_c5',  \n",
    "    input_shape={'input_ids': [1, 128]},  # Example: batch size 1, sequence length 128\n",
    "    output_path=compiled_output_path\n",
    ")\n",
    "\n",
    "print('Compilation job initiated. Check the SageMaker console for job status.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the Compiled Model On-Device\n",
    "\n",
    "After the model is compiled, the optimized artifact is stored in S3. In a real-world scenario, you would download this artifact and integrate it with the runtime environment on your target edge device. The following cell demonstrates how to list and download the compiled model artifact from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "\n",
    "# List objects in the compiled model output path\n",
    "compiled_bucket = bucket\n",
    "compiled_prefix = 'compiled-models/'  # Adjust if necessary\n",
    "\n",
    "response = s3.list_objects_v2(Bucket=compiled_bucket, Prefix=compiled_prefix)\n",
    "\n",
    "if 'Contents' in response:\n",
    "    print('Compiled model artifacts:')\n",
    "    for obj in response['Contents']:\n",
    "        print(obj['Key'])\n",
    "else:\n",
    "    print('No compiled model artifacts found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On-Device Inference Simulation\n",
    "\n",
    "The following code simulates the process of on-device inference using the compiled model artifact. In production, the artifact would be loaded by a device-specific runtime (for example, an ONNX runtime if the artifact is in ONNX format).\n",
    "\n",
    "For our technical support ticket triage use case, the model processes ticket text and classifies it into categories (e.g., **Billing** or **Technical**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudo-code for on-device inference\n",
    "\n",
    "def load_compiled_model(model_path):\n",
    "    # In practice, load the compiled model using the device-specific runtime.\n",
    "    # For example, if the compiled model is in ONNX format, you might use onnxruntime.InferenceSession.\n",
    "    print(f'Loading compiled model from: {model_path}')\n",
    "    # Return a model object (this is a placeholder for the actual model loader)\n",
    "    return None\n",
    "\n",
    "def preprocess_ticket(ticket_text):\n",
    "    # Convert the ticket text into the input format required by the model (e.g., tokenization, padding).\n",
    "    # In practice, use the tokenizer associated with your LLM model.\n",
    "    return {'input_ids': [0] * 128}  # Dummy tokenized input\n",
    "\n",
    "def infer(model, processed_input):\n",
    "    # Run inference on the processed input using the loaded model.\n",
    "    # This function simulates inference and returns dummy probabilities for each category.\n",
    "    simulated_output = {'Billing': 0.2, 'Technical': 0.8}\n",
    "    return simulated_output\n",
    "\n",
    "# Assume the compiled model artifact has been downloaded to a local directory (update this path as needed)\n",
    "local_compiled_model_path = 'compiled_model_artifact'  # Replace with the actual local path\n",
    "\n",
    "# Load the compiled model (simulation)\n",
    "compiled_model_obj = load_compiled_model(local_compiled_model_path)\n",
    "\n",
    "# Example support ticket text\n",
    "ticket_text = 'My internet is down and I cannot connect. Please help.'\n",
    "\n",
    "# Preprocess the ticket text\n",
    "processed_input = preprocess_ticket(ticket_text)\n",
    "\n",
    "# Perform inference (simulation)\n",
    "inference_result = infer(compiled_model_obj, processed_input)\n",
    "\n",
    "print('Inference Result:')\n",
    "print(inference_result)\n",
    "\n",
    "# Determine the predicted category based on the highest score\n",
    "predicted_category = max(inference_result, key=inference_result.get)\n",
    "print(f'Predicted Ticket Category: {predicted_category}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we demonstrated how to compile an LLM model for technical support ticket triage using Amazon SageMaker Neo. The compiled model is optimized for on-device deployment, enabling faster inference on edge devices. This approach can help automate ticket categorization, reduce manual effort, and improve response times.\n",
    "\n",
    "For production use, integrate the compiled artifact with your device-specific runtime environment to achieve real-time inference on edge devices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
