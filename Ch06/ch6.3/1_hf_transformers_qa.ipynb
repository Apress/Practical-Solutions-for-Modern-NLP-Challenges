{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb66c03c",
   "metadata": {},
   "source": [
    "\n",
    "# 6.3.1 - Implementing Question Answering with Hugging Face Transformers\n",
    "\n",
    "This notebook demonstrates how to build both extractive and generative Question Answering (QA) systems using Hugging Face Transformers.\n",
    "\n",
    "We'll cover:\n",
    "- Extractive QA using `DistilBERT` and `DeBERTa`\n",
    "- Generative QA using `T5` and `Mistral-7B-Instruct`\n",
    "- A CPU-efficient LLM (`BitNet-b1.58`) for generative inference\n",
    "\n",
    "Each example includes loading the model, preparing inputs, and running inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d196664",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install transformers datasets accelerate bitsandbytes sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5d8b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import pipeline, AutoModelForQuestionAnswering, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e67301f",
   "metadata": {},
   "source": [
    "## Extractive QA using DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31331ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load pre-trained DistilBERT QA model\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
    "\n",
    "context = \"Hugging Face is a company based in New York that specializes in Natural Language Processing.\"\n",
    "question = \"Where is Hugging Face based?\"\n",
    "\n",
    "# Run inference\n",
    "result = qa_pipeline(question=question, context=context)\n",
    "print(\"Answer:\", result['answer'], \"Score:\", result['score'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8438ca2",
   "metadata": {},
   "source": [
    "## Extractive QA using DeBERTa-v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a73855",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"microsoft/deberta-v3-base\")\n",
    "\n",
    "context = \"Quantum computing uses the principles of quantum mechanics such as superposition and entanglement.\"\n",
    "question = \"What principles does quantum computing use?\"\n",
    "\n",
    "result = qa_pipeline(question=question, context=context)\n",
    "print(\"Answer:\", result['answer'], \"Score:\", result['score'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13637fb6",
   "metadata": {},
   "source": [
    "## Generative QA using T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdf1a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "input_text = \"question: What is the capital of France? context: France is a country in Europe. Its capital is Paris.\"\n",
    "inputs = t5_tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "output = t5_model.generate(**inputs, max_length=32)\n",
    "print(\"Answer:\", t5_tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a518bf09",
   "metadata": {},
   "source": [
    "## Generative QA using Mistral-7B-Instruct (mocked with smaller model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befdf777",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Due to resource constraints, we'll use mistralai/Mistral-7B-Instruct if available\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"tiiuae/falcon-rw-1b\"  # Placeholder for \"mistralai/Mistral-7B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "input_text = \"Q: What is relativity? Context: Relativity is the dependence of various physical phenomena on relative motion.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "output = model.generate(**inputs, max_length=50)\n",
    "print(\"Answer:\", tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76101ffe",
   "metadata": {},
   "source": [
    "## CPU-efficient Generative QA with BitNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db0cd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simulate BitNet (actual model needs custom loader; we show mocked example)\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# We'll mock BitNet with a lightweight model for compatibility\n",
    "model_name = \"sshleifer/tiny-gpt2\"  # Placeholder for BitNet\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "inputs = tokenizer(\"Q: What is quantum computing? Context: Quantum computing uses qubits.\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=50)\n",
    "print(\"Answer:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
