{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7c25519",
   "metadata": {},
   "source": [
    "\n",
    "# Quantizing and Exporting DistilBERT for Mobile and Edge Deployment\n",
    "\n",
    "This notebook demonstrates how to efficiently deploy Small Language Models (SLMs), specifically DistilBERT, using quantization techniques to reduce model size and inference latency. The notebook further explains exporting the quantized model into ONNX format for efficient deployment on mobile devices and edge computing platforms.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b748843",
   "metadata": {},
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d110d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install torch transformers onnx onnxruntime\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9a6d1b",
   "metadata": {},
   "source": [
    "## Load and Quantize DistilBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4649fcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import DistilBertForTokenClassification, DistilBertTokenizer\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Load the fine-tuned DistilBERT model\n",
    "model = DistilBertForTokenClassification.from_pretrained('./results')\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Apply dynamic quantization\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# Save the quantized model\n",
    "quantized_model.save_pretrained('./quantized_model')\n",
    "\n",
    "print(\"Quantization complete. Model saved.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5795a285",
   "metadata": {},
   "source": [
    "## Test Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc746bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenize sample input\n",
    "inputs = tokenizer(\"Apple is looking at buying U.K. startup for $1 billion\", return_tensors=\"pt\")\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    outputs = quantized_model(**inputs)\n",
    "\n",
    "print(\"Model Output:\", outputs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bb2402",
   "metadata": {},
   "source": [
    "## Export Quantized Model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6d7cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import DistilBertForTokenClassification\n",
    "\n",
    "# Reload the quantized model\n",
    "model = DistilBertForTokenClassification.from_pretrained('./quantized_model')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Create a realistic dummy input for exporting (batch size=1, seq length=10)\n",
    "dummy_input = torch.randint(0, tokenizer.vocab_size, (1, 10), dtype=torch.long)\n",
    "\n",
    "# Export to ONNX format\n",
    "torch.onnx.export(\n",
    "    model, \n",
    "    dummy_input, \n",
    "    \"distilbert_ner_model.onnx\", \n",
    "    opset_version=11,\n",
    "    input_names=['input_ids'], \n",
    "    output_names=['logits'],\n",
    "    dynamic_axes={'input_ids': {0: 'batch_size', 1: 'sequence_length'}, 'logits': {0: 'batch_size', 1: 'sequence_length'}}\n",
    ")\n",
    "\n",
    "print(\"Model successfully exported to ONNX format.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2162b457",
   "metadata": {},
   "source": [
    "## Verify ONNX Model with ONNX Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab644c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import onnxruntime as ort\n",
    "\n",
    "# Load ONNX model with ONNX Runtime\n",
    "onnx_session = ort.InferenceSession(\"distilbert_ner_model.onnx\")\n",
    "\n",
    "# Prepare input for ONNX Runtime\n",
    "onnx_inputs = {'input_ids': dummy_input.numpy()}\n",
    "\n",
    "# Perform inference\n",
    "onnx_outputs = onnx_session.run(None, onnx_inputs)\n",
    "\n",
    "print(\"ONNX Model Output:\", onnx_outputs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2818bc3",
   "metadata": {},
   "source": [
    "## Performance Comparison (Optional Benchmarking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f49ab50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "# Measure inference time for PyTorch model\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    _ = quantized_model(dummy_input)\n",
    "pytorch_inference_time = time.time() - start_time\n",
    "\n",
    "# Measure inference time for ONNX model\n",
    "start_time = time.time()\n",
    "_ = onnx_session.run(None, onnx_inputs)\n",
    "onnx_inference_time = time.time() - start_time\n",
    "\n",
    "print(f\"PyTorch Quantized Inference Time: {pytorch_inference_time:.6f} seconds\")\n",
    "print(f\"ONNX Inference Time: {onnx_inference_time:.6f} seconds\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0b0d3f",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated the full workflow from quantizing a fine-tuned DistilBERT model to exporting it as an ONNX model, suitable for deployment in resource-constrained environments such as mobile devices and edge platforms. Quantization significantly reduces model size and improves inference efficiency, making it ideal for real-time Named Entity Recognition (NER) applications.\n",
    "    "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
