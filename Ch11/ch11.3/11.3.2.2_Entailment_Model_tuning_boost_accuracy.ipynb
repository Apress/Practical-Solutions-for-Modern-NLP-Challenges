{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc077bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "# 1. Data Augmentation\n",
    "# This function demonstrates a simple form of data augmentation:\n",
    "# paraphrasing to create new training examples.\n",
    "def augment_data(premise, hypothesis, label):\n",
    "    augmented_data = []\n",
    "    # Paraphrase the premise\n",
    "    paraphrased_premise = premise.replace(\"governed by the laws of\", \"subject to the jurisdiction of\")\n",
    "    augmented_data.append({\"premise\": paraphrased_premise, \"hypothesis\": hypothesis, \"label\": label})\n",
    "    \n",
    "    # Paraphrase the hypothesis\n",
    "    paraphrased_hypothesis = hypothesis.replace(\"subject to the jurisdiction of\", \"governed by the laws of\")\n",
    "    augmented_data.append({\"premise\": premise, \"hypothesis\": paraphrased_hypothesis, \"label\": label})\n",
    "    \n",
    "    return augmented_data\n",
    "\n",
    "# Original data\n",
    "entailment_data = [\n",
    "    {\"premise\": \"This agreement shall be governed by the laws of the State of New York.\",\n",
    "     \"hypothesis\": \"The contract is subject to the jurisdiction of New York.\",\n",
    "     \"label\": \"entailment\"}\n",
    "]\n",
    "\n",
    "# Augment the data\n",
    "print(\"--- üß¨ Data Augmentation for Entailment ---\")\n",
    "augmented_dataset = []\n",
    "for item in entailment_data:\n",
    "    augmented_dataset.extend(augment_data(item['premise'], item['hypothesis'], item['label']))\n",
    "\n",
    "print(\"Original Data:\", entailment_data[0])\n",
    "print(\"Augmented Data:\", augmented_dataset)\n",
    "\n",
    "# 2. Domain-Specific Fine-Tuning\n",
    "# This is the same code from the previous response, but it's explicitly labeled\n",
    "# here as a key technique for \"improving accuracy.\" By training on legal\n",
    "# data, the model learns the specific nuances of legal language.\n",
    "def preprocess_entailment_data(examples):\n",
    "    inputs = [f\"mnli premise: {p} hypothesis: {h}\" for p, h in zip(examples['premise'], examples['hypothesis'])]\n",
    "    outputs = [l for l in examples['label']]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(outputs, max_length=16, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Create a dataset (combining original and augmented data)\n",
    "dataset = Dataset.from_dict({\"premise\": [d['premise'] for d in augmented_dataset],\n",
    "                             \"hypothesis\": [d['hypothesis'] for d in augmented_dataset],\n",
    "                             \"label\": [d['label'] for d in augmented_dataset]})\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_entailment_data, batched=True)\n",
    "\n",
    "# Training arguments for fine-tuning\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./entailment_model_fine_tuned\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=10, # More epochs for better fine-tuning\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_dataset,\n",
    "#     tokenizer=tokenizer,\n",
    "# )\n",
    "\n",
    "# Start the fine-tuning process\n",
    "# print(\"\\n--- ‚öñÔ∏è Domain-Specific Fine-Tuning ---\")\n",
    "# print(\"Starting fine-tuning with a larger, augmented dataset...\")\n",
    "# trainer.train()\n",
    "# print(\"Fine-tuning complete. Model saved to './entailment_model_fine_tuned'.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
