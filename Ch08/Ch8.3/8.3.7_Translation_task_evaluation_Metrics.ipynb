{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebb2ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68de8cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install evaluate transformers bert-score sacrebleu rouge-score --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41d025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Evaluation Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b6bebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import evaluate\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Sample translation data\n",
    "data = [\n",
    "    {\n",
    "        \"source\": \"The weather is nice today.\",\n",
    "        \"reference\": \"Il fait beau aujourd'hui.\"\n",
    "    },\n",
    "    {\n",
    "        \"source\": \"Welcome to our customer service.\",\n",
    "        \"reference\": \"Bienvenue √† notre service client.\"\n",
    "    },\n",
    "    {\n",
    "        \"source\": \"I would like to book a flight to Paris.\",\n",
    "        \"reference\": \"Je voudrais r√©server un vol pour Paris.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# 2. Load a translation pipeline (Helsinki-NLP for en-fr)\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "\n",
    "predictions = []\n",
    "latencies = []\n",
    "\n",
    "# 3. Run translation with latency measurement\n",
    "for item in data:\n",
    "    start_time = time.time()\n",
    "    translated = translator(item[\"source\"], max_length=128)[0][\"translation_text\"]\n",
    "    end_time = time.time()\n",
    "    latency = round((end_time - start_time) * 1000, 2)  # in ms\n",
    "\n",
    "    predictions.append(translated)\n",
    "    latencies.append(latency)\n",
    "\n",
    "# 4. Extract references\n",
    "references = [item[\"reference\"] for item in data]\n",
    "\n",
    "# 5. Load evaluation metrics\n",
    "bleu = evaluate.load(\"sacrebleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "ter = evaluate.load(\"ter\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "# 6. Compute all metrics\n",
    "bleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "rouge_score = rouge.compute(predictions=predictions, references=references)\n",
    "ter_score = ter.compute(predictions=predictions, references=references)\n",
    "bertscore_result = bertscore.compute(predictions=predictions, references=references, lang=\"fr\")\n",
    "\n",
    "# 7. Output summary\n",
    "summary = {\n",
    "    \"BLEU\": bleu_score[\"score\"],\n",
    "    \"ROUGE-L\": rouge_score[\"rougeL\"],\n",
    "    \"TER\": ter_score[\"score\"],\n",
    "    \"BERTScore (F1 avg)\": sum(bertscore_result[\"f1\"]) / len(bertscore_result[\"f1\"]),\n",
    "    \"Avg Latency (ms)\": sum(latencies) / len(latencies)\n",
    "}\n",
    "\n",
    "print(\"\\nüìä Evaluation Summary:\")\n",
    "for k, v in summary.items():\n",
    "    print(f\"{k}: {v:.2f}\")\n",
    "\n",
    "# 8. Optional: Show results in a table\n",
    "df = pd.DataFrame({\n",
    "    \"Source\": [item[\"source\"] for item in data],\n",
    "    \"Reference\": references,\n",
    "    \"Prediction\": predictions,\n",
    "    \"Latency (ms)\": latencies\n",
    "})\n",
    "\n",
    "print(\"\\Per-Sentence Results:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c315666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88638f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A sample output:  Evaluation Summary:\n",
    "BLEU: 41.23\n",
    "ROUGE-L: 0.59\n",
    "TER: 0.31\n",
    "BERTScore (F1 avg): 0.87\n",
    "Avg Latency (ms): 128.45\n",
    "\n",
    " Per-Sentence Results:\n",
    "                                Source                              Reference                           Prediction  Latency (ms)\n",
    "0       The weather is nice today.       Il fait beau aujourd'hui.     Le temps est agr√©able aujourd'hui.        130.24\n",
    "1  Welcome to our customer service.  Bienvenue √† notre service client.   Bienvenue dans notre service client.     127.14\n",
    "2  I would like to book a flight to Paris.  Je voudrais r√©server un vol pour Paris.  Je souhaite r√©server un vol √† Paris.   128.34\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e6050a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# further explore the below for effective perf monitoring\n",
    "\n",
    "1. Add support for multiple languages.\n",
    "\n",
    "2. Visualize metrics with matplotlib or seaborn.\n",
    "\n",
    "3. Integrate with wandb or MLflow for tracking."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
