{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a7aa5d5",
   "metadata": {},
   "source": [
    "# Multilingual Content Delivery with Translation Models on AWS and Hugging Face\n",
    "This notebook demonstrates a full pipeline for multilingual content delivery using machine translation.\n",
    "We cover:\n",
    "- Detecting the source language\n",
    "- Translating content using pre-trained models\n",
    "- Evaluating translation quality\n",
    "- Storing translations\n",
    "- Using LLM agent logic to decide translation strategies\n",
    "\n",
    "We use Hugging Face Transformers, Langdetect for language detection, and simulate AWS integrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56da10eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries (if not already installed)\n",
    "!pip install transformers sentencepiece langdetect --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9820da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from langdetect import detect\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "import random\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a6cd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample content in different languages\n",
    "content_items = [\n",
    "    {\"id\": 1, \"title\": \"Bienvenue à Paris\", \"body\": \"Paris est la capitale de la France, connue pour sa culture et son histoire.\"},\n",
    "    {\"id\": 2, \"title\": \"Willkommen in Berlin\", \"body\": \"Berlin ist die Hauptstadt von Deutschland und eine Stadt voller Kunst.\"},\n",
    "    {\"id\": 3, \"title\": \"Welcome to New York\", \"body\": \"New York is a bustling city in the United States, famous for its skyline.\"},\n",
    "    {\"id\": 4, \"title\": \"स्वागत है मुंबई में\", \"body\": \"मुंबई भारत का एक प्रमुख शहर है, जो अपने सिनेमा और व्यापार के लिए प्रसिद्ध है।\"},\n",
    "    {\"id\": 5, \"title\": \"Bem-vindo ao Rio de Janeiro\", \"body\": \"Rio é uma cidade maravilhosa no Brasil, famosa pelo carnaval.\"}\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(content_items)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9fc5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect language using langdetect\n",
    "def detect_language(text: str) -> str:\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "\n",
    "df[\"language_detected\"] = df[\"body\"].apply(detect_language)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01159ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load translation model: English as target language\n",
    "model_name = \"Helsinki-NLP/opus-mt-{}-en\"\n",
    "\n",
    "# Available source languages\n",
    "supported_langs = [\"fr\", \"de\", \"hi\", \"pt\"]  # French, German, Hindi, Portuguese\n",
    "\n",
    "translator_pipelines = {}\n",
    "\n",
    "for lang in supported_langs:\n",
    "    try:\n",
    "        pipeline_obj = pipeline(\"translation\", model=model_name.format(lang))\n",
    "        translator_pipelines[lang] = pipeline_obj\n",
    "    except Exception as e:\n",
    "        print(f\"Model for '{lang}' could not be loaded: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb62879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate content to English\n",
    "def translate_to_english(text: str, lang: str) -> str:\n",
    "    if lang in translator_pipelines:\n",
    "        return translator_pipelines[lang](text)[0][\"translation_text\"]\n",
    "    return \"Translation not available\"\n",
    "\n",
    "df[\"translated_body\"] = df.apply(lambda row: translate_to_english(row[\"body\"], row[\"language_detected\"]), axis=1)\n",
    "df[[\"id\", \"language_detected\", \"translated_body\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69d5060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a basic evaluation (BLEU, ROUGE would require ref translations)\n",
    "# Here we simulate confidence by language match and sentence length\n",
    "\n",
    "def estimate_quality(original: str, translated: str) -> float:\n",
    "    if translated == \"Translation not available\":\n",
    "        return 0.0\n",
    "    len_ratio = len(translated) / (len(original) + 1e-6)\n",
    "    ratio_score = max(0.0, min(1.0, 1.0 - abs(1 - len_ratio)))\n",
    "    return round(ratio_score, 2)\n",
    "\n",
    "df[\"quality_score\"] = df.apply(lambda row: estimate_quality(row[\"body\"], row[\"translated_body\"]), axis=1)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423392fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save translated content to a JSON file\n",
    "output_data = df[[\"id\", \"title\", \"translated_body\", \"language_detected\", \"quality_score\"]].to_dict(orient=\"records\")\n",
    "\n",
    "with open(\"translated_content.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Translated content saved to translated_content.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faad82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate an LLM agent-like translation strategy\n",
    "def agent_translate(content: str, lang: str, threshold: float = 0.6) -> Tuple[str, str]:\n",
    "    if lang not in supported_langs:\n",
    "        return \"Unsupported language\", \"fallback\"\n",
    "    translation = translate_to_english(content, lang)\n",
    "    quality = estimate_quality(content, translation)\n",
    "    strategy = \"high_quality_model\" if quality >= threshold else \"human_review_required\"\n",
    "    return translation, strategy\n",
    "\n",
    "# Apply to all rows\n",
    "df[[\"agent_translation\", \"strategy\"]] = df.apply(\n",
    "    lambda row: pd.Series(agent_translate(row[\"body\"], row[\"language_detected\"])), axis=1\n",
    ")\n",
    "\n",
    "df[[\"id\", \"language_detected\", \"strategy\", \"agent_translation\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd6fc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary report\n",
    "summary = df.groupby(\"strategy\").size().reset_index(name=\"count\")\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adefb48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save entire dataframe to CSV for further analysis\n",
    "df.to_csv(\"multilingual_translation_report.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce19f38",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we demonstrated a multilingual content delivery system using Hugging Face translation models, language detection, quality estimation, and LLM agent-style logic to adapt strategy per sentence. This can be extended to incorporate AWS Translate, store in S3, or serve via SageMaker endpoints."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
