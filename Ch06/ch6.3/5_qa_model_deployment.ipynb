{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5313f557",
   "metadata": {},
   "source": [
    "\n",
    "# 6.3.5 - Deployment of QA Models\n",
    "\n",
    "This notebook covers QA model deployment strategies for both cloud and edge environments.\n",
    "\n",
    "Topics covered:\n",
    "- Exporting a Hugging Face model to ONNX\n",
    "- Using AWS Lambda for serverless inference\n",
    "- CPU-efficient deployment with BitNet (mock example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36a6e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install transformers onnx onnxruntime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576334f0",
   "metadata": {},
   "source": [
    "## Export Hugging Face QA Model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0ee1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from transformers.onnx import export\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "onnx_path = Path(\"onnx_qa\")\n",
    "onnx_path.mkdir(exist_ok=True)\n",
    "export(tokenizer, model=model, output=onnx_path/\"qa_model.onnx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a68450e",
   "metadata": {},
   "source": [
    "## Run Inference with ONNX Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b000853",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import onnxruntime as ort\n",
    "\n",
    "session = ort.InferenceSession(\"onnx_qa/qa_model.onnx\")\n",
    "\n",
    "question = \"What is the capital of Italy?\"\n",
    "context = \"Italy is a European country. Rome is the capital of Italy.\"\n",
    "\n",
    "inputs = tokenizer(question, context, return_tensors=\"np\", padding=\"max_length\", truncation=True, max_length=384)\n",
    "outputs = session.run(None, dict(inputs))\n",
    "start, end = outputs[0].argmax(), outputs[1].argmax()\n",
    "answer = tokenizer.decode(inputs[\"input_ids\"][0][start:end])\n",
    "print(\"ONNX Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916aabcd",
   "metadata": {},
   "source": [
    "\n",
    "## AWS Lambda Deployment Overview\n",
    "\n",
    "You can package your QA model and deploy it on AWS Lambda using the following:\n",
    "1. Convert to `torchscript` or `onnx`\n",
    "2. Bundle model and inference script in a zip\n",
    "3. Create a Lambda function with appropriate memory\n",
    "4. Use API Gateway to expose it\n",
    "\n",
    "**Key Python handler snippet:**\n",
    "```python\n",
    "def lambda_handler(event, context):\n",
    "    question = event['question']\n",
    "    context_str = event['context']\n",
    "    # load tokenizer/model and return answer span\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4f3307",
   "metadata": {},
   "source": [
    "\n",
    "## BitNet Deployment on CPU\n",
    "\n",
    "BitNet is optimized for CPU with 1.58-bit quantization and GGUF format.\n",
    "\n",
    "### CLI Steps:\n",
    "```bash\n",
    "python convert_bitnet_to_gguf.py\n",
    "quantize --qtype Q1_58\n",
    "./bitnet -m qa_model.gguf -p \"Q: Who wrote Hamlet?\"\n",
    "```\n",
    "\n",
    "Due to BitNet's custom format, we use a placeholder here in notebook for demo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd12ab27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"sshleifer/tiny-gpt2\"  # mock for BitNet\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"Q: What is the speed of light? Context: Light travels at 299,792,458 meters per second.\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "output_ids = model.generate(input_ids, max_length=64)\n",
    "print(\"BitNet-mock Answer:\", tokenizer.decode(output_ids[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
