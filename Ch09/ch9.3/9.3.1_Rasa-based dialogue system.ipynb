{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7790b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a custom Rasa NLU component that leverages a Hugging Face model for intent classification, a core component of a Rasa-based dialogue system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3721ce03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a Rasa custom NLU component that integrates with Hugging Face Transformers\n",
    "# The code below should be placed in a custom Python file (e.g., hf_nlu.py)\n",
    "# and configured in your Rasa pipeline in config.yml.\n",
    "\n",
    "import logging\n",
    "from typing import Any, Text, Dict, List, Type\n",
    "\n",
    "from rasa.nlu.components import Component\n",
    "from rasa.nlu.classifiers.classifier import IntentClassifier\n",
    "from rasa.nlu.training_data import Message, TrainingData\n",
    "from rasa.nlu.model import Metadata\n",
    "\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class HuggingFaceIntentClassifier(IntentClassifier, Component):\n",
    "    \"\"\"A custom intent classifier using Hugging Face's DistilBERT.\"\"\"\n",
    "\n",
    "    # Name of the component\n",
    "    name = \"HuggingFaceIntentClassifier\"\n",
    "\n",
    "    # Defines the required components in the pipeline before this one\n",
    "    requires = []\n",
    "\n",
    "    # Defines the components that can follow this one\n",
    "    provides = [\"intent\", \"intent_ranking\"]\n",
    "\n",
    "    # Defines the defaults for the configuration\n",
    "    defaults = {\n",
    "        \"model_name\": \"distilbert-base-uncased\",\n",
    "        \"label_mapping\": {},\n",
    "        \"epochs\": 5,\n",
    "        \"batch_size\": 16\n",
    "    }\n",
    "\n",
    "    # Initializes the component\n",
    "    def __init__(self, component_config: Dict[Text, Any] = None) -> None:\n",
    "        super(HuggingFaceIntentClassifier, self).__init__(component_config)\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.label_mapping = None\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, component_config: Dict[Text, Any], config: Dict[Text, Any]) -> \"HuggingFaceIntentClassifier\":\n",
    "        \"\"\"Creates a new instance of the component.\"\"\"\n",
    "        return cls(component_config)\n",
    "\n",
    "    def train(\n",
    "        self, training_data: TrainingData, config: Dict[Text, Any], **kwargs: Any\n",
    "    ) -> None:\n",
    "        \"\"\"Train this component on a given training data.\"\"\"\n",
    "        # Get the unique intents from the training data\n",
    "        intents = [e.data[\"intent\"] for e in training_data.intent_examples]\n",
    "        unique_intents = sorted(list(set(intents)))\n",
    "        self.label_mapping = {intent: i for i, intent in enumerate(unique_intents)}\n",
    "        \n",
    "        # Load the pre-trained DistilBERT tokenizer and model\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(self.component_config[\"model_name\"])\n",
    "        self.model = TFDistilBertForSequenceClassification.from_pretrained(\n",
    "            self.component_config[\"model_name\"], num_labels=len(unique_intents)\n",
    "        )\n",
    "\n",
    "        # Prepare training data\n",
    "        texts = [e.text for e in training_data.intent_examples]\n",
    "        labels = [self.label_mapping[e.data[\"intent\"]] for e in training_data.intent_examples]\n",
    "\n",
    "        # Tokenize the texts and convert to TensorFlow dataset\n",
    "        tokenized_texts = self.tokenizer(texts, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(\n",
    "            (\n",
    "                dict(tokenized_texts),\n",
    "                labels\n",
    "            )\n",
    "        ).shuffle(len(texts)).batch(self.component_config[\"batch_size\"])\n",
    "\n",
    "        # Compile and train the model\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "        self.model.compile(optimizer=optimizer, loss=self.model.compute_loss)\n",
    "\n",
    "        logger.info(f\"Training DistilBERT model for {self.component_config['epochs']} epochs...\")\n",
    "        self.model.fit(dataset, epochs=self.component_config[\"epochs\"])\n",
    "\n",
    "    def process(self, message: Message, **kwargs: Any) -> None:\n",
    "        \"\"\"Process an incoming message and classify its intent.\"\"\"\n",
    "        if not self.model or not self.tokenizer or not self.label_mapping:\n",
    "            logger.warning(\"Model not trained. Skipping intent classification.\")\n",
    "            return\n",
    "\n",
    "        # Tokenize the input message\n",
    "        inputs = self.tokenizer(\n",
    "            [message.text], \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            return_tensors=\"tf\"\n",
    "        )\n",
    "        \n",
    "        # Get predictions from the model\n",
    "        predictions = self.model(inputs)\n",
    "        logits = predictions.logits\n",
    "        \n",
    "        # Get the intent with the highest probability\n",
    "        predicted_label = tf.argmax(logits, axis=1).numpy()[0]\n",
    "        confidence = tf.nn.softmax(logits, axis=1).numpy()[0]\n",
    "        \n",
    "        # Map the label back to the original intent name\n",
    "        intent_name = list(self.label_mapping.keys())[list(self.label_mapping.values()).index(predicted_label)]\n",
    "        \n",
    "        # Create a ranking of all intents\n",
    "        intent_ranking = [\n",
    "            {\"name\": intent, \"confidence\": float(confidence[self.label_mapping[intent]])}\n",
    "            for intent in self.label_mapping.keys()\n",
    "        ]\n",
    "        intent_ranking = sorted(intent_ranking, key=lambda x: x[\"confidence\"], reverse=True)\n",
    "\n",
    "        # Update the message with the classified intent and ranking\n",
    "        message.set(\"intent\", {\"name\": intent_name, \"confidence\": float(confidence[predicted_label])}, add_to_output=True)\n",
    "        message.set(\"intent_ranking\", intent_ranking, add_to_output=True)\n",
    "\n",
    "    def persist(self, file_name: Text, model_dir: Text) -> Dict[Text, Any]:\n",
    "        \"\"\"Persist this model into the passed directory.\"\"\"\n",
    "        # Save the tokenizer and model\n",
    "        save_path = os.path.join(model_dir, file_name)\n",
    "        self.model.save_pretrained(save_path)\n",
    "        self.tokenizer.save_pretrained(save_path)\n",
    "        \n",
    "        # Save the label mapping as well\n",
    "        with open(os.path.join(save_path, \"label_mapping.json\"), \"w\") as f:\n",
    "            json.dump(self.label_mapping, f)\n",
    "\n",
    "        # Return a dictionary with the filename and a reference to the saved files\n",
    "        return {\"file\": file_name, \"label_mapping_file\": \"label_mapping.json\"}\n",
    "\n",
    "    @classmethod\n",
    "    def load(\n",
    "        cls,\n",
    "        meta: Dict[Text, Any],\n",
    "        model_dir: Text,\n",
    "        component_config: Dict[Text, Any],\n",
    "        **kwargs: Any\n",
    "    ) -> \"HuggingFaceIntentClassifier\":\n",
    "        \"\"\"Load a trained model from the storage.\"\"\"\n",
    "        # Load the tokenizer and model\n",
    "        save_path = os.path.join(model_dir, meta[\"file\"])\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(save_path)\n",
    "        model = TFDistilBertForSequenceClassification.from_pretrained(save_path)\n",
    "        \n",
    "        # Load the label mapping\n",
    "        with open(os.path.join(save_path, meta[\"label_mapping_file\"]), \"r\") as f:\n",
    "            label_mapping = json.load(f)\n",
    "\n",
    "        # Create and return the component instance\n",
    "        component = cls(component_config)\n",
    "        component.tokenizer = tokenizer\n",
    "        component.model = model\n",
    "        component.label_mapping = label_mapping\n",
    "        return component\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800f96c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To use the custom Hugging Face intent classifier for inference in Rasa, follow these steps after implementing the component code in above referenced git code (e.g., in hf_nlu.py):\n",
    "\n",
    "# config.yml \n",
    "\n",
    "language: en\n",
    "\n",
    "pipeline:\n",
    "  - name: WhitespaceTokenizer\n",
    "  - name: CountVectorsFeaturizer\n",
    "  - name: custom_components.hf_nlu.HuggingFaceIntentClassifier\n",
    "    model_name: \"distilbert-base-uncased\"\n",
    "    epochs: 5\n",
    "    batch_size: 16\n",
    "\n",
    "\n",
    "#Make sure the import path (e.g., custom_components.hf_nlu.HuggingFaceIntentClassifier) matches your project structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5275bc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "rasa train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76843c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step4: Run the Rasa Shell for Inference\n",
    "rasa shell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7373519c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5:  Programmatic Inference\n",
    "#If you want to use the model outside of Rasa (e.g., for batch inference or testing), you can do something like this:\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Load model, tokenizer, and label mapping\n",
    "MODEL_DIR = \"models/nlu/custom_component\"  # Adjust based on persist location\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_DIR)\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(MODEL_DIR)\n",
    "\n",
    "with open(os.path.join(MODEL_DIR, \"label_mapping.json\")) as f:\n",
    "    label_mapping = json.load(f)\n",
    "\n",
    "inv_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "# Inference function\n",
    "def classify_intent(text):\n",
    "    inputs = tokenizer([text], return_tensors=\"tf\", truncation=True, padding=True)\n",
    "    logits = model(inputs).logits\n",
    "    probs = tf.nn.softmax(logits, axis=1).numpy()[0]\n",
    "    predicted_label = tf.argmax(logits, axis=1).numpy()[0]\n",
    "    return {\n",
    "        \"intent\": inv_label_mapping[predicted_label],\n",
    "        \"confidence\": float(probs[predicted_label]),\n",
    "        \"ranking\": [\n",
    "            {\"intent\": inv_label_mapping[i], \"confidence\": float(conf)}\n",
    "            for i, conf in sorted(enumerate(probs), key=lambda x: x[1], reverse=True)\n",
    "        ]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758b8977",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classify_intent(\"I want to book a flight\"))\n",
    "\n",
    "#output\n",
    "\n",
    "\n",
    "#\n",
    "# The classifier modifies the Message object with two keys: intent and intent_ranking.\n",
    "{\n",
    "  \"intent\": {\n",
    "    \"name\": \"book_flight\",\n",
    "    \"confidence\": 0.92},\n",
    "  \"intent_ranking\": [\n",
    "    {\"name\": \"book_flight\",\n",
    "      \"confidence\": 0.92},\n",
    "    {\"name\": \"inform_location\",\n",
    "      \"confidence\": 0.05},\n",
    "    {\"name\": \"greet\",\n",
    "      \"confidence\": 0.02},\n",
    "    {\"name\": \"cancel\",\n",
    "      \"confidence\": 0.01} ]}\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
