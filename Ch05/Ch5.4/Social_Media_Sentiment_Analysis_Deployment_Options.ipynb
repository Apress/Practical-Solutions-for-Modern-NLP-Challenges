{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 778
    },
    "id": "XmukQSUNnqn_",
    "outputId": "5fe5a393-2415-4a67-d33f-22ff43959d54"
   },
   "outputs": [],
   "source": [
    "#!pip install transformers[torch]\n",
    "\n",
    "!pip install transformers==4.28.1\n",
    "import transformers\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cgiCKZARsBsM",
    "outputId": "e35b9b09-2913-44e8-d843-1e0115227f48"
   },
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368,
     "referenced_widgets": [
      "f0f7a3de4a584f49ab4c7cd1836e05b6",
      "457f6c9949e54769ab14a01eebbb8e01",
      "b74c87c5363245f5b4a42fb137c38d82",
      "b1639e6ff65c44b9936ee3666388b70a",
      "c09fb079b3ac4dfcadcd314d2cea072d",
      "1e7163950a7746f381ef0a42f88e9d14",
      "984b95836ace4e39b86cea927ca46911",
      "c2a586eb826d4a61a05d5507884621fe",
      "ccea4f4731b3437888e7f2d59bf20041",
      "7141e16ef31c45bcbe81de181d6b0787",
      "dfb5595a41a64f1dbe536456f2d0fc08",
      "b380e764866643b89498b0f58f1e4794",
      "75a9c1af81f04ecf89686931addc3434",
      "28b1be3ae81644f49367f3767e94a6fc",
      "80ed8f6ca70544cfa2ba8511561a2c49",
      "7dd514faf03f4728a281f8ce8c1fbafd",
      "a23394449967445a8a36a1b58ca13779",
      "bfe114e295844b8d8a8bcc5cde9b2d72",
      "f15e5151314f4882a2519050d1310c8b",
      "0edeb81dc3f04c33a787e1f1d70c10a7",
      "87f141bc9e9f41b98099f2d1abefae0b",
      "312c8e172a4b4ab5af7c29845f4babd7",
      "533c28c3c2104fd18b6bf4468beca492",
      "3078f468a8fa4472bbac5cd2dc9d296d",
      "48c2feee4a5b470cbd606682a82a9600",
      "e3f4859725dc449ab7bd43f929ba4496",
      "44df3742065b4a3bb225f474f7845594",
      "cc4d04ae72674264b9e48bae562ba998",
      "1b785379e6b74c7a87f50e51bbff1cd4",
      "b75c51bbb602497faf25d79b2e2632d3",
      "1c601bcb41304e1f87992f26c1ae3f4d",
      "e2d20ab6e01043d9849912e786b94a81",
      "7b526ec4a351414489b9415054b19222",
      "5495a0b2f27f48fca4826a93b0bff01f",
      "5a703d04f00447458c10a48106281d4f",
      "cf87bfb6e33c42f5957bc8878941b5a1",
      "9ee8646146b34040ad8a57905bbfa543",
      "5d183f8dd03d4760ae32cd67c20911e3",
      "89b168a520634423a561a43f59dd37f0",
      "ff74077a6242419691b7f4a733e1c9a7",
      "85fcb4e47b504bf1887af46cbcf045f8",
      "d39906a511064142b344866a2dd7c53d",
      "e0a64b1e85b3407eaf699212a54a8854",
      "b889b02952854b2887a9cf2305b03051",
      "de52384d74ce43a8a540bb09309041c2",
      "ef16bcb1122a45029dfd26878e1d9119",
      "4fa7d45476a7423d9c62d8299d56f9b7",
      "b8626148d54246c99f2c4403a37b7999",
      "8cb8330cff4a4d99a6bb51f9ad43f70b",
      "bc2d38ec439748b6aa58699508247880",
      "65e82d3b23bb46d186ecfa8e793d9d80",
      "186d86394aeb44799b850db8d542f261",
      "6cc4ed9c3373497b9e087956051e274b",
      "19dc045126194903b0a3a9b1f7a56b87",
      "23435ca65e3c43fea1b941ef97440f9d",
      "d4226db059c140248e32e438185b6a20",
      "bc6988edbf4e4b5faaad226643811549",
      "c2e19ad9315d42ae896b5abe238a8b50",
      "b774d02d73e2491a919306da2b3445af",
      "a6cbdb66359746b6b4c3c5c2a6537a0c",
      "95971b409d1c4df5a6cce12ca1b567ff",
      "d2738f5353154907a27995304c7c9e0a",
      "f1ebd18ddea6488b927f596d546f4fc0",
      "89c9c19bc4ff4b79ae703c5acb7685e3",
      "61c5cca50429472cb9ed89adad79b0f8",
      "1ce091c6146e4a98a873920e0849b49f",
      "3a0036a36bbc46e9b74e47cb891ae93b",
      "8742ab3ffad5453cab884206d905a77f",
      "077170e62a2c427ca4d62eb8a11f542f",
      "f7191086fbaf4e25abf68b99fa8129a0",
      "6102172e0ae1412fac3b613160779b14",
      "3f3a203961e843558cb254d08cddb541",
      "394c724bb2924e17ad7cf78a5cd91c4a",
      "d713e046c98e45979a59f4e9d8ec6ada",
      "60a64c346cb44a319660c60b30af8d5e",
      "0dfd69006c5c466cae41e7f8d9e2e744",
      "777c2f88eeb24e259aff1e56d28a4855"
     ]
    },
    "id": "83sew_aEpj-G",
    "outputId": "886a2c21-1958-4703-c127-7e6780185a50"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "# Load IMDb dataset\n",
    "imdb_dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sfxxveHXpQxN"
   },
   "outputs": [],
   "source": [
    "### Summary of Sentiment Analysis steps ###\n",
    "\n",
    "# Data Loading and Preprocessing: Loads the IMDb movie review dataset, creates a custom dataset class, and splits the data into training and validation sets.\n",
    "# Model Initialization: Initializes a pre-trained BERT model for sequence classification and moves it to the appropriate device (GPU if available, otherwise CPU).\n",
    "# Training and Evaluation: Defines the training parameters, initializes a Trainer object, trains the model, and saves it to a directory.\n",
    "# Prediction: Defines functions to prepare input text and predict sentiment. An example is given to predict the sentiment of a movie review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MztQLW02pMBg"
   },
   "outputs": [],
   "source": [
    "# Shuffle the training dataset and select the first 10,000 records\n",
    "train_dataset = imdb_dataset['train'].shuffle(seed=42).select(range(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361,
     "referenced_widgets": [
      "3b0be1039d394c0285a84fae7bfe050e",
      "cb5d63b40fd8411ab2814f4c7cf3deb7",
      "de3548f0c08a48f592c0bd90b2ad4b7a",
      "ef7dd0335d0d41ed9874d476ca772819",
      "61a974f2830d45c1b656a105eff31aaa",
      "6c7e40f8fc33411296747f546019c535",
      "b5174a950b184b4697d3ff709911c3e9",
      "3aeba4c1014a467183debb437b2b4059",
      "f78e243439b847f79ea41b79159724a5",
      "e8ec84d92fdb4d8d9872577b856e6f5c",
      "bdd1729029ea4063a291679ecc4bc5b3",
      "703797f8cdab4533b92ccb123515ab07",
      "10590522ef504dd2942ab1e12ced8427",
      "7d216dd503834bd4aa5560fd513b3801",
      "aab72485675b4cfab8884e595c6dc1a9",
      "869b2b12f3df48a1970852fd6c6f046a",
      "bd41cdefdf6941979eb118b66c4ef1cf",
      "cd5aaf4f3efa4db7b43a3a39dd1bdda7",
      "506556d6f73d4afd8d917b0096ed2b8f",
      "2b70eb81fe0d4e0284a719053c6c85af",
      "76748b3385ff4c61881003e6d97b91e3",
      "27f8ffe60e7c46dd98071f8ccc1fcac1",
      "85f2c5546a0046818619b7c63fe2176a",
      "483b53a1a5cc4b209ae5ced477965772",
      "9bde375e81bd4caca823c973256d5155",
      "57a8391882f84437928ef356f0c6df19",
      "f6f886f757a74a6c919e0206f957b40e",
      "ac5730c59397445594ec627b79a3f2c7",
      "2b13e544d158448aab3a733500c75a84",
      "15d85e0084c04abc93217b4d656e431c",
      "b6b7329135fc41b0a74075b53c93683d",
      "90be6c9510d742fcaa73433bb6602e3e",
      "ad11773a77834804862fe77c53061092",
      "33f9dbea4bf848a78de032e867bd468e",
      "a66ee241cb34442dadff39738c8d8caf",
      "aa6911baba4d45ec90d3d695c3cfe0ad",
      "3e75253613464240b094caf5148d603a",
      "24efa268c4da4d8d951ad7d38d0ab11a",
      "bdf50804c90b4d8fb7ca587d35ec1aa9",
      "350b8a543f9047cc894034a96abfee4e",
      "841a626dd8c540e1a8815e2dd2fcdd18",
      "c634f661fce840e4b2aac75d16c95151",
      "1b3ceae827ce45d4af8e22101c5f5601",
      "7cc4291e29bc4d539fff9ac90d14a14a"
     ]
    },
    "id": "3pYcB_fCnvh5",
    "outputId": "592a4204-14e8-4649-f906-52c16820ae70"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "# Determine if a GPU is available and use it\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Extract texts and labels\n",
    "texts = train_dataset['text']\n",
    "labels = [1 if label == 1 else 0 for label in train_dataset['label']]\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define dataset and dataloaders\n",
    "train_dataset = CustomDataset(train_texts, train_labels, tokenizer, max_length=128)\n",
    "val_dataset = CustomDataset(val_texts, val_labels, tokenizer, max_length=128)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Initialize BERT model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.eval()\n",
    "total_val_loss = 0\n",
    "num_val_steps = 0\n",
    "for batch in val_dataloader:\n",
    "    with torch.no_grad():\n",
    "        # Move each batch to GPU\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_val_loss += loss.item()\n",
    "        num_val_steps += 1\n",
    "\n",
    "avg_val_loss = total_val_loss / num_val_steps\n",
    "print(f\"Validation loss: {avg_val_loss}\")\n",
    "\n",
    "\n",
    "tokenizer.save_pretrained('./final_model')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "skYnqtBHn5mn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8vq7ZqcJnyo-"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./model_output\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=500,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=500,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    load_best_model_at_end=True  # Load the best model at the end\n",
    ")\n",
    "\n",
    "# Initialize the Trainer with the GPU-enabled model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset  # Optional, for validation\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# save the model to a directory\n",
    "model.save_pretrained(\"./final_model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QrhQO4EUn6tv"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_path = './final_model'\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Make sure to evaluate the model\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prepare_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def predict_sentiment(text):\n",
    "    # Prepare the text\n",
    "    inputs = prepare_text(text)\n",
    "\n",
    "    # Move inputs to the same device as model\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    # Prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        prediction = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "    # Assuming the classes are [0, 1] where '0' is negative and '1' is positive\n",
    "    classes = ['Negative', 'Positive']\n",
    "    predicted_class = classes[prediction.argmax()]\n",
    "    return predicted_class, prediction[0].tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AYMYr-NXn8u_"
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "review = \"A phenomenal film, blending mesmerizing visuals with outstanding character development.\"\n",
    "sentiment, scores = predict_sentiment(review)\n",
    "print(f\"Sentiment: {sentiment}, Scores: {scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6799zCiuqNph"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8zigXuTCqNsu"
   },
   "outputs": [],
   "source": [
    "# Deployment options in AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bA8kKYTjqce4"
   },
   "outputs": [],
   "source": [
    "# 1. Sagemaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-pbx6CQ6qVvF"
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# Create a HuggingFaceModel object\n",
    "model = HuggingFaceModel(\n",
    "    model_data=\"s3://<your-bucket>/final_model.tar.gz\", # Replace with your model path\n",
    "    role=\"<your-iam-role>\", # Replace with your IAM role\n",
    "    transformers_version=\"4.17\",\n",
    "    pytorch_version=\"1.9\",\n",
    "    py_version=\"py38\",\n",
    ")\n",
    "\n",
    "# Deploy the model to a SageMaker endpoint\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "predictions = predictor.predict({\"inputs\": \"This movie was great!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vC4uGMiOqjS4"
   },
   "outputs": [],
   "source": [
    "# 2. AWS Elastic Beanstalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_gW23M38qkD6"
   },
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load your model and tokenizer\n",
    "model_path = \"./final_model\"\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "\n",
    "@app.route(\"/\", methods=[\"POST\"])\n",
    "def predict():\n",
    "    data = request.get_json()\n",
    "    text = data[\"text\"]\n",
    "    # Preprocess and predict\n",
    "    # ...\n",
    "    return jsonify({\"sentiment\": sentiment})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True, host=\"0.0.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "57sct90vqnUy"
   },
   "outputs": [],
   "source": [
    "# 3. AWS Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D9onjRnnqpD5"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Load your model and tokenizer\n",
    "model_path = \"./final_model\"\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    text = event[\"text\"]\n",
    "    # Preprocess and predict\n",
    "    # ...\n",
    "    return {\"sentiment\": sentiment}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4WUycfzsqq4H"
   },
   "outputs": [],
   "source": [
    "# open source deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FWr5hvb1rDy7"
   },
   "outputs": [],
   "source": [
    "# 1. Deploying as a Web Service with Flask or FastAPI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y2nK9hGcrG6i"
   },
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from transformers import pipeline\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load sentiment analysis pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"path/to/your/model\")\n",
    "\n",
    "@app.route(\"/predict\", methods=[\"POST\"])\n",
    "def predict():\n",
    "    text = request.json.get(\"text\")\n",
    "    result = classifier(text)[0]\n",
    "    return jsonify({\"sentiment\": result[\"label\"], \"score\": result[\"score\"]})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xvtDLs9NrJBL"
   },
   "outputs": [],
   "source": [
    "# 2. Using Docker for Containerization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "80YrPfOnrP06"
   },
   "outputs": [],
   "source": [
    "FROM python:3.9\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "COPY . .\n",
    "\n",
    "CMD [\"flask\", \"run\", \"--host=0.0.0.0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uyVu566jrRoL"
   },
   "outputs": [],
   "source": [
    "# 3. Deploying to Platforms like Heroku or Google Cloud Run\n",
    "\n",
    "# Create an account on the platform.\n",
    "# Push your Docker image to the platform's registry.\n",
    "# Deploy the image as a web service.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xEArxCY_rTSK"
   },
   "outputs": [],
   "source": [
    "# 4. Using Open-Source Model Serving Frameworks like TorchServe or TensorFlow Serving:\n",
    "\n",
    "# Package your model in the required format for the framework.\n",
    "# Configure the framework to serve your model.\n",
    "# Deploy the framework on a server.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
