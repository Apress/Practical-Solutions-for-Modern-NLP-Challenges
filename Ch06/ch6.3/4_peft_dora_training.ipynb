{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1696aea",
   "metadata": {},
   "source": [
    "\n",
    "# 6.3.4 - Next-Gen Fine-Tuning with PEFT (DoRA)\n",
    "\n",
    "This notebook demonstrates how to fine-tune a large language model using parameter-efficient methods such as DoRA (Decomposed and Recomposed Attention).\n",
    "\n",
    "We will:\n",
    "- Load a model and dataset\n",
    "- Use the `peft` library with `LoraConfig`\n",
    "- Enable DoRA for efficient adaptation\n",
    "- Train using the `trl` library (SFTTrainer)\n",
    "\n",
    "> Note: This example uses a small model for demonstration. Replace with larger models in production.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a793c76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install transformers datasets peft trl accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdc13e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e28117",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = load_dataset(\"squad\")\n",
    "sample = dataset[\"train\"].select(range(100))  # use small subset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34007a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"tiiuae/falcon-rw-1b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fe3102",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_qa(example):\n",
    "    question = example[\"question\"]\n",
    "    context = example[\"context\"]\n",
    "    answer = example[\"answers\"][\"text\"][0]\n",
    "    return {\"text\": f\"question: {question} context: {context} answer: {answer}\"}\n",
    "\n",
    "formatted_dataset = sample.map(format_qa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c14875d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=64,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    use_dora=True,\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50750b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=formatted_dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=512,\n",
    "    dataset_text_field=\"text\",\n",
    "    args={\n",
    "        \"per_device_train_batch_size\": 4,\n",
    "        \"num_train_epochs\": 1,\n",
    "        \"logging_steps\": 10,\n",
    "        \"output_dir\": \"./dora-peft-model\"\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84c8a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3315d0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.save_pretrained(\"./dora-peft-model\")\n",
    "tokenizer.save_pretrained(\"./dora-peft-model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08f6608",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = \"question: What is the capital of Germany? context: Germany is a country in Europe. Its capital is Berlin. answer:\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "output_ids = model.generate(input_ids, max_length=64)\n",
    "print(\"Answer:\", tokenizer.decode(output_ids[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
