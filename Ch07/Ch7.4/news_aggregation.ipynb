{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85b9c3aa",
   "metadata": {},
   "source": [
    "# 7.4.1 News Aggregation\n",
    "\n",
    "This notebook demonstrates a **News Aggregation** pipeline leveraging both extractive summarization (SLMs) and abstractive summarization (LLMs) to turn multiple articles on a topic into concise, informative summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5010d04d",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "Weâ€™ll use:\n",
    "- **newspaper3k** to fetch and parse online articles  \n",
    "- **Sumy** for extractive summarization (LexRank)  \n",
    "- **Transformers** for abstractive summarization (BART & PEGASUS)  \n",
    "- **NLTK** for tokenization support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fda90a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install newspaper3k sumy nltk transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b42482b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # required by Sumy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c25024",
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fec9bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Sample news article URLs (you can swap these for any RSS-derived list) --\n",
    "urls = [\n",
    "    \"https://www.bbc.com/news/world-us-canada-65878956\",\n",
    "    \"https://www.bbc.com/news/technology-65872304\",\n",
    "    \"https://www.bbc.com/news/business-65870012\"\n",
    "]\n",
    "\n",
    "def fetch_article_text(url):\n",
    "    art = Article(url)\n",
    "    art.download()\n",
    "    art.parse()\n",
    "    return art.text\n",
    "\n",
    "# Fetch and combine\n",
    "articles = [fetch_article_text(u) for u in urls]\n",
    "combined_text = \"\\n\\n\".join(articles)\n",
    "print(f\"Fetched {len(articles)} articles, total characters: {len(combined_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb03d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractive_summary(text: str, num_sentences: int = 3) -> str:\n",
    "    parser    = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
    "    summarizer = LexRankSummarizer()\n",
    "    summary    = summarizer(parser.document, num_sentences)\n",
    "    return \" \".join(str(s) for s in summary)\n",
    "\n",
    "ext_sum = extractive_summary(combined_text, num_sentences=5)\n",
    "print(\"=== Extractive Summary (LexRank, 5 sentences) ===\\n\")\n",
    "print(ext_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16403af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Hugging Face summarization pipelines\n",
    "bart_summarizer    = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "pegasus_summarizer = pipeline(\"summarization\", model=\"google/pegasus-xsum\")\n",
    "\n",
    "def abstractive_summary(text: str, model: str = \"bart\",\n",
    "                        max_length: int = 150, min_length: int = 40) -> str:\n",
    "    if model == \"bart\":\n",
    "        return bart_summarizer(text,\n",
    "                               max_length=max_length,\n",
    "                               min_length=min_length,\n",
    "                               do_sample=False)[0][\"summary_text\"]\n",
    "    elif model == \"pegasus\":\n",
    "        return pegasus_summarizer(text,\n",
    "                                  max_length=max_length,\n",
    "                                  min_length=min_length,\n",
    "                                  do_sample=False)[0][\"summary_text\"]\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {model}\")\n",
    "\n",
    "abs_bart    = abstractive_summary(combined_text, model=\"bart\")\n",
    "abs_pegasus = abstractive_summary(combined_text, model=\"pegasus\")\n",
    "\n",
    "print(\"=== Abstractive Summary (BART) ===\\n\", abs_bart, \"\\n\")\n",
    "print(\"=== Abstractive Summary (PEGASUS) ===\\n\", abs_pegasus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1eb5a8",
   "metadata": {},
   "source": [
    "## 2. Analysis & Next Steps\n",
    "\n",
    "- **Extractive vs. Abstractive**: The LexRank summary is guaranteed to be fact-faithful (it only picks sentences), while BART/PEGASUS can rephrase and remove redundancies but may hallucinate.  \n",
    "- **Scaling Up**: Swap the static URL list for an RSS-feed reader loop or a news API to ingest hundreds of articles.  \n",
    "- **Clustering**: Before summarizing, cluster articles by similarity (e.g., with a vector store) to generate topic-specific summaries.  \n",
    "- **Deployment**:  \n",
    "  - **Local**: Expose via a Flask/FastAPI endpoint.  \n",
    "  - **Cloud**: Containerize with Docker; deploy on AWS/GCP with autoscaling.  \n",
    "  - **Real-Time**: Integrate with streaming platforms (Kafka, Pub/Sub) for live summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b876bd",
   "metadata": {},
   "source": [
    "## 3. Conclusion\n",
    "\n",
    "In this use case, we built a simple yet powerful News Aggregation system:\n",
    "1. **Fetched** real-world articles.  \n",
    "2. **Summarized** them extractively with Sumy (LexRank).  \n",
    "3. **Generated** fluent abstractive summaries with BART & PEGASUS.  \n",
    "\n",
    "This pattern can be extended with clustering, reranking, custom fine-tuning, or pipeline orchestration to power real-time news dashboards and alerting systems."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
