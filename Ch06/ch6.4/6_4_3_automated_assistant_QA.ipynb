{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Section 6.4.3: Automated Assistant QA System\n", "\n", "This notebook demonstrates how to build an automated assistant using extractive and generative QA techniques.\n", "- Extractive QA: Uses DistilBERT to locate answers from context.\n", "- Generative QA: Uses T5 model to generate answers.\n", "- Vector store: Uses ChromaDB with HuggingFace embeddings.\n", "\n", "**Use case**: IT or HR assistant answering enterprise-level queries."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Install required libraries (Uncomment and run if not installed)\n", "# !pip install transformers datasets chromadb langchain\n", "# !pip install sentence-transformers\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from transformers import DistilBertTokenizerFast, DistilBertForQuestionAnswering, pipeline\n", "from transformers import T5Tokenizer, T5ForConditionalGeneration\n", "from langchain.vectorstores import Chroma\n", "from langchain.embeddings import HuggingFaceEmbeddings\n", "from langchain.text_splitter import RecursiveCharacterTextSplitter\n", "from langchain.document_loaders import TextLoader\n", "from langchain.chains import RetrievalQA\n", "from langchain.llms import HuggingFacePipeline\n", "import os"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 1: Prepare documents\n", "Assume we have some IT/HR knowledge documents to index and query from."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Sample document creation\n", "doc_text = \"\"\"\n", "Q: How to reset your corporate email password?\n", "A: You can reset your password by visiting the internal IT portal and clicking on 'Reset Password'.\n", "\n", "Q: What is the leave policy for new employees?\n", "A: New employees are entitled to 20 days of annual leave, starting from their date of joining.\n", "\n", "Q: Who do I contact for hardware issues?\n", "A: Please reach out to the IT Helpdesk at helpdesk@company.com for hardware-related problems.\n", "\"\"\"\n", "with open(\"knowledge_base.txt\", \"w\") as f:\n", "    f.write(doc_text)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 2: Create vector store from documents using HuggingFace embeddings"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Load and split the document\n", "loader = TextLoader(\"knowledge_base.txt\")\n", "docs = loader.load()\n", "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n", "split_docs = text_splitter.split_documents(docs)\n", "\n", "# Create embeddings\n", "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n", "vectordb = Chroma.from_documents(split_docs, embedding_model, persist_directory=\"chroma_store\")\n", "vectordb.persist()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 3: Setup Extractive QA with DistilBERT"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Load DistilBERT for QA\n", "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n", "question = \"How can I reset my email password?\"\n", "context = split_docs[0].page_content\n", "result = qa_pipeline(question=question, context=context)\n", "print(\"Extractive QA Answer:\", result['answer'])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 4: Setup Generative QA with T5 model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Load T5 for Generative QA\n", "t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n", "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n", "\n", "input_text = f\"question: {question} context: {context}\"\n", "input_ids = t5_tokenizer.encode(input_text, return_tensors=\"pt\")\n", "output_ids = t5_model.generate(input_ids, max_length=50)\n", "answer = t5_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n", "print(\"Generative QA Answer:\", answer)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 5: Retrieval-Augmented QA using Langchain + Chroma + T5"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from transformers import pipeline as hf_pipeline\n", "\n", "qa_gen_pipe = hf_pipeline(\"text2text-generation\", model=\"t5-small\")\n", "llm = HuggingFacePipeline(pipeline=qa_gen_pipe)\n", "retriever = vectordb.as_retriever()\n", "\n", "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, chain_type=\"stuff\")\n", "result = qa_chain.run(\"Who to contact for hardware issues?\")\n", "print(\"Hybrid QA Answer:\", result)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}}, "nbformat": 4, "nbformat_minor": 5}