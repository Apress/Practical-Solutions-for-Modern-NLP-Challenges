{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d99a0971",
   "metadata": {},
   "source": [
    "# LLM-SLM Hybrid QA Router\n",
    "This notebook demonstrates a routing mechanism using AWS Lambda to dynamically direct user questions to either a lightweight SLM model or a more powerful LLM endpoint depending on question complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1af45d",
   "metadata": {},
   "source": [
    "## Step 1: Define Simple Routing Logic Based on Question Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6601c8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_question(question: str) -> str:\n",
    "    if \"how\" in question.lower() or len(question.split()) > 15:\n",
    "        return \"LLM\"\n",
    "    else:\n",
    "        return \"SLM\"\n",
    "\n",
    "# Example\n",
    "sample_qs = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"How does AWS SageMaker work with Transformers models for QA tasks?\"\n",
    "]\n",
    "\n",
    "for q in sample_qs:\n",
    "    print(f\"Question: {q} -> Route: {classify_question(q)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d589fed2",
   "metadata": {},
   "source": [
    "## Step 2: Define Lambda Handler for Hybrid Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fded13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import torch\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "# Load SLM model in Lambda container\n",
    "tokenizer_slm = AutoTokenizer.from_pretrained(\"deepset/minilm-uncased-squad2\")\n",
    "model_slm = AutoModelForQuestionAnswering.from_pretrained(\"deepset/minilm-uncased-squad2\")\n",
    "\n",
    "# Optional: mock Bedrock/GPT-4 response for demonstration\n",
    "def mock_bedrock_response(question, context):\n",
    "    return f\"[LLM Answer] for: {question}\"\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    body = json.loads(event.get(\"body\", \"{}\"))\n",
    "    question = body.get(\"question\")\n",
    "    context_text = body.get(\"context\")\n",
    "\n",
    "    if classify_question(question) == \"SLM\":\n",
    "        inputs = tokenizer_slm(question, context_text, return_tensors='pt')\n",
    "        outputs = model_slm(**inputs)\n",
    "        answer_start = torch.argmax(outputs.start_logits)\n",
    "        answer_end = torch.argmax(outputs.end_logits) + 1\n",
    "        answer = tokenizer_slm.convert_tokens_to_string(\n",
    "            tokenizer_slm.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end])\n",
    "        )\n",
    "    else:\n",
    "        # Call to Bedrock or other LLM here\n",
    "        answer = mock_bedrock_response(question, context_text)\n",
    "\n",
    "    return {\n",
    "        \"statusCode\": 200,\n",
    "        \"body\": json.dumps({\"answer\": answer})\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05002aa5",
   "metadata": {},
   "source": [
    "## Step 3: (Optional) Replace Mock with Real Bedrock Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1c4a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Bedrock client call (works only if you have permissions and access):\n",
    "# bedrock = boto3.client(\"bedrock-runtime\")\n",
    "# response = bedrock.invoke_model(\n",
    "#     modelId=\"anthropic.claude-v2\",\n",
    "#     contentType=\"application/json\",\n",
    "#     body=json.dumps({\"prompt\": f\"Answer this question: {question} \\nContext: {context}\"})\n",
    "# )\n",
    "# result = json.loads(response['body'].read().decode())\n",
    "# return result['completion']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863bb369",
   "metadata": {},
   "source": [
    "## Step 4: Deploying the Lambda Router (Terraform Reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5c8136",
   "metadata": {},
   "outputs": [],
   "source": [
    "terraform_code = '''\n",
    "resource \"aws_lambda_function\" \"qa_router_lambda\" {\n",
    "  function_name = \"qa-router\"\n",
    "  role          = aws_iam_role.lambda_exec.arn\n",
    "  package_type  = \"Image\"\n",
    "  image_uri     = \"<ecr_image_with_router_logic>\"\n",
    "  timeout       = 15\n",
    "  memory_size   = 1024\n",
    "}\n",
    "'''\n",
    "print(terraform_code)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
