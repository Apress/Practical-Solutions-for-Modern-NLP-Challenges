{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "699976b3",
   "metadata": {},
   "source": [
    "# ðŸ“„ Business Document Summarization Pipeline\n",
    "This notebook demonstrates a complete pipeline to summarize business documents such as reports, meeting notes, and contracts using transformer-based models. It supports batch processing and saves results in structured format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d72ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd8f325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "from transformers import pipeline\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1fa214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_table_name(filename):\n",
    "    match = re.match(r'^([a-zA-Z0-9_]+?)_\\d+', filename)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return filename  # fallback if no match\n",
    "\n",
    "# Example usage\n",
    "filename = \"table_abc_17988\"\n",
    "table_name = extract_table_name(filename)\n",
    "print(table_name\n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b588138",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Loading summarization model...\")\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92d7e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Remove excess whitespace and normalize text.\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddaceca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text: str, min_length=40, max_length=150) -> str:\n",
    "    \"\"\"Generate a summary from input text.\"\"\"\n",
    "    result = summarizer(text, max_length=max_length, min_length=min_length, do_sample=False)\n",
    "    return result[0]['summary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cdfd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_file(filepath: str) -> Dict[str, str]:\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        raw = f.read()\n",
    "    cleaned = clean_text(raw)\n",
    "    summary = summarize_text(cleaned)\n",
    "    return {\"filename\": os.path.basename(filepath), \"summary\": summary}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7cd128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup input directory\n",
    "input_dir = \"./business_docs\"\n",
    "os.makedirs(input_dir, exist_ok=True)\n",
    "print(f\"[INFO] Place .txt files in: {input_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7b7782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample file for demonstration\n",
    "sample_content = '''\n",
    "Q2 Strategy Meeting:\n",
    "The sales team presented a plan to expand into Asia-Pacific with support from the product team. Budget increases are expected for international marketing campaigns.\n",
    "Compliance raised concerns over data localization laws in Singapore and India. Legal advised engaging local counsel for contracts. All action items are due next quarter.\n",
    "'''\n",
    "sample_path = os.path.join(input_dir, \"q2_strategy_notes.txt\")\n",
    "with open(sample_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(sample_content)\n",
    "print(f\"[INFO] Created sample file: {sample_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a40f17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch summarization logic\n",
    "def summarize_directory(directory: str) -> List[Dict[str, str]]:\n",
    "    summaries = []\n",
    "    for file_path in glob.glob(os.path.join(directory, '*.txt')):\n",
    "        print(f\"Summarizing: {file_path}\")\n",
    "        summary = summarize_file(file_path)\n",
    "        print(summary['summary'])\n",
    "        summaries.append(summary)\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55805770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to JSON\n",
    "results = summarize_directory(input_dir)\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_file = f\"summarized_documents_{timestamp}.json\"\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "print(f\"[INFO] Saved summaries to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c4f7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tuning Fine-tuning DistilBERT with PEFT (LoRA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375d5f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# --- Load Dataset ---\n",
    "# Using CNN/DailyMail for demonstration. In an enterprise setting, you'd load your own data.\n",
    "# For true extractive summarization, your dataset needs sentence-level labels.\n",
    "# This example will adapt CNN/DailyMail for a simpler classification-like task.\n",
    "try:\n",
    "    dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train[:10000]\") # Load a smaller subset for faster demo\n",
    "    eval_dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"validation[:1000]\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load cnn_dailymail dataset: {e}. Please ensure you have internet access or specify a local path.\")\n",
    "    print(\"Loading a dummy dataset for demonstration purposes.\")\n",
    "    # Create a dummy dataset if loading fails\n",
    "    from datasets import Dataset\n",
    "    dummy_data = {\n",
    "        \"article\": [\n",
    "            \"This is the first article about business. It talks about financial growth and market trends. The economy is expanding quickly.\",\n",
    "            \"Another document discusses legal contracts. It highlights clauses related to data privacy and regulatory compliance. This is very important.\"\n",
    "        ],\n",
    "        \"highlights\": [\n",
    "            \"Financial growth is a key topic. Economy is expanding.\",\n",
    "            \"Legal contracts focus on data privacy.\"\n",
    "        ]\n",
    "    }\n",
    "    dataset = Dataset.from_dict(dummy_data)\n",
    "    eval_dataset = Dataset.from_dict(dummy_data) # Use dummy for eval too\n",
    "\n",
    "# --- Load Tokenizer ---\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# --- Preprocessing Function ---\n",
    "# For extractive summarization, you'd typically predict if each sentence is a summary sentence.\n",
    "# Here, for demonstration with CNN/DailyMail, we'll simplify.\n",
    "# We'll tokenize the article and set a dummy label as this model expects labels for sequence classification.\n",
    "# In a real extractive scenario, you'd have labels indicating summary sentences.\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize the article\n",
    "    model_inputs = tokenizer(examples[\"article\"], max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    # In a real extractive task, 'labels' would be derived from sentence-level annotations.\n",
    "    # For this demonstration, we'll create dummy labels for sequence classification.\n",
    "    # This assumes a simplified scenario where the model predicts *something* about the whole article.\n",
    "    # For robust extractive summarization, you need to classify each sentence.\n",
    "    # E.g., for each sentence, is_summary = 1 or 0. This typically involves a different model head.\n",
    "    # For now, we'll just set a dummy label (e.g., first token ID or a constant) if labels are required by the model,\n",
    "    # or you'd use a different model type for token/sequence classification on sentences.\n",
    "    # Since DistilBertForSequenceClassification expects a single label per sequence:\n",
    "    # We'll just use a placeholder. In a real scenario, you'd classify based on the content.\n",
    "    # A simple approach for a dummy label might be to hash the highlight text or use a constant.\n",
    "    # For demonstration, let's just make sure there's a 'label' field.\n",
    "    # If using for pure unsupervised feature extraction, you might not need labels for initial stages.\n",
    "    # For supervised fine-tuning, you must have labels.\n",
    "\n",
    "    # Let's create a dummy binary label for demonstration:\n",
    "    # If the highlights contain specific keywords (e.g., \"financial\"), set label to 1, else 0.\n",
    "    # This is *not* how extractive summarization is truly labeled, but it demonstrates the structure.\n",
    "    model_inputs[\"labels\"] = [1 if \"financial\" in h.lower() else 0 for h in examples[\"highlights\"]]\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "# Apply preprocessing\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "tokenized_eval_dataset = eval_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Remove original text columns to save memory if not needed by the model directly\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"article\", \"highlights\"])\n",
    "tokenized_eval_dataset = tokenized_eval_dataset.remove_columns([\"article\", \"highlights\"])\n",
    "\n",
    "# Set format for PyTorch\n",
    "tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "tokenized_eval_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "print(\"--- Dataset Prepared ---\")\n",
    "print(f\"Training dataset size: {len(tokenized_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(tokenized_eval_dataset)}\")\n",
    "print(tokenized_dataset[0]) # Example of a tokenized entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6cc11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Fine-tuning DistilBERT with PEFT (LoRA)\n",
    "# We'll use Parameter-Efficient Fine-Tuning (PEFT), specifically LoRA, to adapt DistilBERT to our (simulated) extractive summarization task. This significantly reduces the computational resources required for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad28945",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# --- Load DistilBERT Model for Sequence Classification ---\n",
    "# For true extractive summarization, you might use DistilBertForTokenClassification\n",
    "# to predict a label for each token (e.g., part of summary sentence).\n",
    "# DistilBertForSequenceClassification is used here for simplicity with dummy labels.\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2) # 2 labels for our dummy binary classification\n",
    "\n",
    "# --- Define LoRA Configuration ---\n",
    "# Target modules are usually the attention query and value projections.\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS, # Specify the task type\n",
    "    r=64,                      # The rank of the update matrices\n",
    "    lora_alpha=16,             # The scaling factor for LoRA\n",
    "    lora_dropout=0.1,          # Dropout probability for LoRA layers\n",
    "    bias=\"none\",               # Bias can be 'none', 'all', or 'lora_only'\n",
    "    target_modules=[\"q_lin\", \"k_lin\", \"v_lin\"], # Common target modules for BERT-like models\n",
    ")\n",
    "\n",
    "# Apply LoRA to the base model\n",
    "model = get_peft_model(model, peft_config)\n",
    "print(\"\\n--- PEFT Model Trainable Parameters ---\")\n",
    "model.print_trainable_parameters() # Shows how many parameters are actually being trained (very few!)\n",
    "\n",
    "# --- Training Arguments ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./distilbert-summarization-peft\", # Directory to save checkpoints\n",
    "    num_train_epochs=3,                          # Number of training epochs\n",
    "    per_device_train_batch_size=8,               # Batch size per GPU/CPU for training\n",
    "    per_device_eval_batch_size=8,                # Batch size per GPU/CPU for evaluation\n",
    "    warmup_steps=500,                            # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,                           # Strength of weight decay\n",
    "    logging_dir=\"./logs\",                        # Directory for storing logs\n",
    "    logging_steps=100,                           # Log every X updates steps\n",
    "    evaluation_strategy=\"epoch\",                 # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",                       # Save model at the end of each epoch\n",
    "    load_best_model_at_end=True,                 # Load the best model after training\n",
    "    metric_for_best_model=\"eval_loss\",           # Metric to monitor for best model\n",
    "    push_to_hub=False,                           # Don't push to Hugging Face Hub for enterprise internal models\n",
    ")\n",
    "\n",
    "# --- Initialize Trainer ---\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    #tokenizer=tokenizer, # Pass tokenizer to trainer for padding/truncation during evaluation if needed\n",
    ")\n",
    "\n",
    "# --- Train the Model ---\n",
    "print(\"\\n--- Starting Model Training ---\")\n",
    "trainer.train()\n",
    "print(\"\\n--- Model Training Complete ---\")\n",
    "\n",
    "# --- Save the fine-tuned LoRA adapter ---\n",
    "# This saves only the small LoRA weights, not the entire DistilBERT model.\n",
    "# You can then load the base DistilBERT and add these adapters to it.\n",
    "model.save_pretrained(\"./distilbert-summarization-peft/lora_adapter\")\n",
    "tokenizer.save_pretrained(\"./distilbert-summarization-peft/tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95138990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Deployment with FastAPI\n",
    "# Now, let's set up a basic REST API using FastAPI to serve our fine-tuned DistilBERT model. This allows other enterprise applications to easily integrate with the summarization service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335a1334",
   "metadata": {},
   "outputs": [],
   "source": [
    "#app.py\n",
    "\n",
    "# app.py\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from peft import PeftModel, PeftConfig\n",
    "import torch\n",
    "\n",
    "app = FastAPI(title=\"Enterprise Extractive Summarization API\")\n",
    "\n",
    "# --- Load Model and Tokenizer ---\n",
    "# Load the base model\n",
    "base_model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(base_model_name)\n",
    "base_model = DistilBertForSequenceClassification.from_pretrained(base_model_name, num_labels=2) # Ensure num_labels matches training\n",
    "\n",
    "# Load the PEFT adapter\n",
    "# Make sure this path points to where you saved your LoRA adapter\n",
    "lora_adapter_path = \"./distilbert-summarization-peft/lora_adapter\"\n",
    "try:\n",
    "    model = PeftModel.from_pretrained(base_model, lora_adapter_path)\n",
    "    model = model.merge_and_unload() # Merge LoRA weights into the base model for faster inference\n",
    "    print(\"LoRA adapter loaded and merged successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load LoRA adapter from {lora_adapter_path}: {e}\")\n",
    "    print(\"Running with base DistilBERT model (without PEFT fine-tuning).\")\n",
    "    model = base_model # Fallback to base model if adapter fails to load\n",
    "\n",
    "model.eval() # Set model to evaluation mode\n",
    "\n",
    "# --- Define Request Body Schema ---\n",
    "class Article(BaseModel):\n",
    "    text: str\n",
    "\n",
    "# --- Summarization Endpoint ---\n",
    "@app.post(\"/summarize/\")\n",
    "def create_summary(article: Article):\n",
    "    \"\"\"\n",
    "    Analyzes an input article to identify key sentences for an extractive summary.\n",
    "    This model performs a classification task on the input,\n",
    "    so the output 'summary' will be based on the model's classification logic,\n",
    "    which in our demo was a dummy binary prediction.\n",
    "    For a real extractive model, you would pass sentences and get scores.\n",
    "    \"\"\"\n",
    "    input_text = article.text\n",
    "\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        # For a sequence classification model, outputs.logits will be raw scores.\n",
    "        # For our dummy example, it's a binary prediction.\n",
    "        # In a true extractive model, you'd process sentence embeddings and classify.\n",
    "        predicted_class = torch.argmax(outputs.logits, dim=-1).item()\n",
    "\n",
    "    # --- SIMULATED EXTRACTIVE SUMMARY LOGIC ---\n",
    "    # This part is highly dependent on how your extractive model is designed.\n",
    "    # For a real extractive model, you would typically:\n",
    "    # 1. Split the input_text into sentences.\n",
    "    # 2. Process each sentence through your model to get a \"summary score\" or classification.\n",
    "    # 3. Select the top-N scoring sentences to form the summary.\n",
    "\n",
    "    # For this demonstration with a sequence classification model trained on dummy labels:\n",
    "    # We'll just return a message indicating the classification.\n",
    "    # Replace this with your actual extractive logic based on your fine-tuned model's output.\n",
    "    if predicted_class == 1:\n",
    "        # If our dummy classifier predicts 1 (e.g., \"contains financial keywords\")\n",
    "        return {\"summary_type\": \"Extractive Classification Result\", \"message\": \"This article seems to be highly relevant or contains key information.\", \"prediction_label\": predicted_class}\n",
    "    else:\n",
    "        return {\"summary_type\": \"Extractive Classification Result\", \"message\": \"This article is less relevant or contains less critical information.\", \"prediction_label\": predicted_class}\n",
    "\n",
    "# To run this API, save the above code as `app.py` and then execute:\n",
    "# uvicorn app:app --reload --host 0.0.0.0 --port 8000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af243e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Running the API\n",
    "# Save the app.py file in the same directory where you ran the fine-tuning script (so it can find the lora_adapter directory).\n",
    "# Open your terminal or command prompt in that directory.\n",
    "# Run the FastAPI application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70f8f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "uvicorn app:app --reload --host 0.0.0.0 --port 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a7b59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Testing the API\n",
    "# Once the API is running, you can test it using a tool like curl or by visiting the interactive API documentation provided by FastAPI.\n",
    "\n",
    "# Access FastAPI Docs: Open your web browser and go to http://127.0.0.1:8000/docs (or your chosen host/port). You'll see the Swagger UI where you can interact with your API.\n",
    "\n",
    "# Using curl (Example):\n",
    "# Open a new terminal and send a POST request.\n",
    "\n",
    "curl -X POST \"http://127.0.0.1:8000/summarize/\" \\\n",
    "     -H \"Content-Type: application/json\" \\\n",
    "     -d '{\"text\": \"This is a very important business report. It discusses our financial performance in the last quarter, highlighting strong revenue growth and significant cost savings. The board meeting minutes also confirmed these positive results and outlined future strategic investments. Our next steps involve expanding into new markets and optimizing our supply chain further.\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a16590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np # Import numpy to check version and ensure it's loaded correctly\n",
    "from datasets import load_dataset, Dataset, DatasetDict # Import DatasetDict\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# --- Critical: Ensure compatible NumPy version in Colab ---\n",
    "# This MUST be at the very top of your Colab notebook and run before anything else.\n",
    "# Restart runtime after running this cell.\n",
    "try:\n",
    "    import numpy as np\n",
    "    if np.__version__.startswith(\"2.\"):\n",
    "        print(\"Downgrading NumPy to a stable 1.x version...\")\n",
    "        !pip install \"numpy<2\" --force-reinstall\n",
    "        import numpy as np # Re-import numpy to load the new version\n",
    "        print(f\"NumPy version after re-installation: {np.__version__}\")\n",
    "    else:\n",
    "        print(f\"NumPy version: {np.__version__} (already compatible)\")\n",
    "except ImportError:\n",
    "    print(\"NumPy not found. Installing compatible version...\")\n",
    "    !pip install \"numpy<2\" --force-reinstall\n",
    "    import numpy as np\n",
    "    print(f\"NumPy version after installation: {np.__version__}\")\n",
    "\n",
    "\n",
    "# --- Critical: Update transformers library ---\n",
    "# Run this after NumPy fix and before other imports if you still get TypeError for TrainingArguments\n",
    "# !pip install --upgrade transformers\n",
    "\n",
    "# --- 1. Data Preparation (Conceptual with Synthetic Fallback) ---\n",
    "print(\"--- Starting Data Preparation ---\")\n",
    "\n",
    "# Try to load CNN/DailyMail dataset; fall back to synthetic data if failed.\n",
    "try:\n",
    "    # A more robust way to load a subset of a large dataset if direct slicing in split fails.\n",
    "    # We'll load the full splits and then select a small portion for demonstration.\n",
    "    # For actual training, remove the [:N] slicing.\n",
    "    full_dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "    dataset = full_dataset[\"train\"].select(range(10000)) # Load 10,000 training examples\n",
    "    eval_dataset = full_dataset[\"validation\"].select(range(1000)) # Load 1,000 validation examples\n",
    "    print(\"Successfully loaded CNN/DailyMail dataset.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load cnn_dailymail dataset: {e}.\")\n",
    "    print(\"Loading a dummy synthetic dataset for demonstration purposes.\")\n",
    "    # Create a synthetic dummy dataset if loading from Hugging Face fails.\n",
    "    dummy_data = {\n",
    "        \"article\": [\n",
    "            \"This is the first article about business. It talks about financial growth and market trends. The economy is expanding quickly. Our revenues increased by 15% this quarter.\",\n",
    "            \"Another document discusses legal contracts. It highlights clauses related to data privacy and regulatory compliance. This is very important for our new GDPR strategy.\",\n",
    "            \"Meeting minutes from yesterday cover product development updates. The team decided to prioritize feature X and de-prioritize feature Y. Next steps include a design review.\",\n",
    "            \"A comprehensive research report on renewable energy. It presents findings on solar panel efficiency and wind turbine performance. Government policies are supporting this sector.\",\n",
    "            \"Internal communication regarding the new HR policy. It details changes to vacation days and sick leave. Employees should review the updated handbook.\",\n",
    "            \"Financial statements for Q3 show strong profits. Despite challenges, our investments are yielding high returns.\",\n",
    "            \"A legal brief on intellectual property rights. It covers patent applications and trademark protection.\",\n",
    "            \"Summary of competitor analysis: Competitor A launched a new product; Competitor B is expanding market share. We need to adapt quickly.\"\n",
    "        ],\n",
    "        \"highlights\": [\n",
    "            \"Financial growth is a key topic. Economy is expanding. Revenues increased by 15%.\",\n",
    "            \"Legal contracts focus on data privacy and regulatory compliance, important for GDPR.\",\n",
    "            \"Meeting minutes cover product development updates, prioritizing feature X.\",\n",
    "            \"Research report on solar panel and wind turbine performance in renewable energy.\",\n",
    "            \"New HR policy details changes to vacation and sick leave.\",\n",
    "            \"Q3 financial statements show strong profits from investments.\",\n",
    "            \"Legal brief covers patent applications and trademark protection.\",\n",
    "            \"Competitor A launched a new product; Competitor B is expanding market share.\"\n",
    "        ]\n",
    "    }\n",
    "    dataset = Dataset.from_dict(dummy_data)\n",
    "    eval_dataset = Dataset.from_dict(dummy_data) # Use dummy for eval too\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Preprocessing Function\n",
    "# For extractive summarization, 'labels' would typically be sentence-level classifications\n",
    "# (e.g., 1 for summary sentence, 0 otherwise).\n",
    "# Here, for demonstration with this dataset structure, we create dummy binary labels.\n",
    "# This simulates a simplified classification task based on keywords in highlights.\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize the article text.\n",
    "    model_inputs = tokenizer(examples[\"article\"], max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    # Create a dummy binary label for demonstration purposes.\n",
    "    # If \"financial\" is in the highlights, assign label 1; otherwise, 0.\n",
    "    # In a real extractive summarization system, 'labels' would precisely mark summary sentences.\n",
    "    model_inputs[\"labels\"] = [1 if \"financial\" in h.lower() else 0 for h in examples[\"highlights\"]]\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "# Apply preprocessing to datasets\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "tokenized_eval_dataset = eval_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Remove original text columns to save memory and ensure correct format for model input\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"article\", \"highlights\"])\n",
    "tokenized_eval_dataset = tokenized_eval_dataset.remove_columns([\"article\", \"highlights\"])\n",
    "\n",
    "# Set format for PyTorch tensors\n",
    "tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "tokenized_eval_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "print(\"--- Dataset Prepared ---\")\n",
    "print(f\"Training dataset size: {len(tokenized_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(tokenized_eval_dataset)}\")\n",
    "if len(tokenized_dataset) > 0:\n",
    "    # Use .with_format(None) temporarily to print a plain Python dict for easier viewing\n",
    "    print(f\"Example tokenized entry: {tokenized_dataset.with_format(None)[0]}\")\n",
    "else:\n",
    "    print(\"No entries in the tokenized training dataset.\")\n",
    "\n",
    "\n",
    "# --- 2. Fine-tuning DistilBERT with PEFT (LoRA) ---\n",
    "print(\"\\n--- Starting Model Loading and PEFT Setup ---\")\n",
    "\n",
    "# Load DistilBERT Model for Sequence Classification\n",
    "# num_labels=2 for our dummy binary classification (relevant/not relevant)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Define LoRA Configuration\n",
    "# TaskType.SEQ_CLS is appropriate for sequence classification.\n",
    "# target_modules specify which layers LoRA should modify.\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=64,                      # Rank of the update matrices\n",
    "    lora_alpha=16,             # Scaling factor for LoRA\n",
    "    lora_dropout=0.1,          # Dropout probability for LoRA layers\n",
    "    bias=\"none\",               # No bias for LoRA layers\n",
    "    target_modules=[\"q_lin\", \"k_lin\", \"v_lin\"], # Standard target modules for DistilBERT attention\n",
    ")\n",
    "\n",
    "# Apply LoRA to the base model\n",
    "model = get_peft_model(model, peft_config)\n",
    "print(\"\\n--- PEFT Model Trainable Parameters ---\")\n",
    "model.print_trainable_parameters() # Displays the small number of trainable parameters\n",
    "\n",
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./distilbert-summarization-peft\", # Directory to save model checkpoints\n",
    "    num_train_epochs=3,                          # Number of training epochs\n",
    "    per_device_train_batch_size=8,               # Batch size per device for training\n",
    "    per_device_eval_batch_size=8,                # Batch size per device for evaluation\n",
    "    warmup_steps=500,                            # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,                           # Strength of weight decay regularization\n",
    "    logging_dir=\"./logs\",                        # Directory for storing logs\n",
    "    logging_steps=100,                           # Log every 100 update steps\n",
    "    #evaluation_strategy=\"epoch\",                 # Evaluate at the end of each epoch\n",
    "    #save_strategy=\"epoch\",                       # Save model checkpoint at the end of each epoch\n",
    "    #load_best_model_at_end=True,                 # Load the best model based on eval_loss after training\n",
    "    metric_for_best_model=\"eval_loss\",           # Metric to monitor for selecting the best model\n",
    "    label_names=[\"labels\"],                      # Explicitly tell Trainer the name of the label column\n",
    "    push_to_hub=False,                           # Do not push the model to Hugging Face Hub (for internal enterprise use)\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,\n",
    "    processing_class=tokenizer, # Pass tokenizer for handling dynamic padding/truncation during evaluation (resolves FutureWarning)\n",
    ")\n",
    "\n",
    "# Train the Model\n",
    "print(\"\\n--- Starting Model Training ---\")\n",
    "trainer.train()\n",
    "print(\"\\n--- Model Training Complete ---\")\n",
    "\n",
    "# Save the fine-tuned LoRA adapter and tokenizer\n",
    "# This saves only the small LoRA weights, which can be loaded onto a base DistilBERT model.\n",
    "output_dir = \"./distilbert-summarization-peft/lora_adapter\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f\"\\n--- LoRA adapter and tokenizer saved to {output_dir} ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ca8289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the above cell code above as train_summarizer.py\n",
    "# python train_summarizer.py\n",
    "# Save the following code as app.py in the same directory as train_summarizer.py:\n",
    "\n",
    "# To run the FastAPI application:\n",
    "\n",
    "# Make sure the train_summarizer.py script has successfully run and created the lora_adapter directory.\n",
    "# Save the app.py code in the same directory.\n",
    "# Open a new terminal or command prompt, navigate to that directory.\n",
    "# Execute the FastAPI application:\n",
    "# uvicorn app:app --reload --host 0.0.0.0 --port 8000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e1142e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Testing the API\n",
    "# Once the FastAPI server is running (you'll see a message like \"Uvicorn running on [suspicious link removed]\"), you can test it.\n",
    "\n",
    "# Using FastAPI's Interactive Docs:\n",
    "\n",
    "# Open your web browser and go to http://127.0.0.1:8000/docs.\n",
    "# You'll see the Swagger UI. Click on the /summarize/ endpoint, then \"Try it out\", and enter some text in the text field. Click \"Execute\".\n",
    "# Using curl (from a new terminal):\n",
    "# You can send POST requests to the API.\n",
    "\n",
    "# Example for \"financial\" related text:\n",
    "\n",
    "# Bash\n",
    "\n",
    "curl -X POST \"http://127.0.0.1:8000/summarize/\" \\\n",
    "     -H \"Content-Type: application/json\" \\\n",
    "     -d '{\"text\": \"Our company had excellent financial performance this quarter. Revenue grew substantially, and we achieved significant cost reductions through efficiency improvements.\"}'\n",
    "Example for non-\"financial\" related text:\n",
    "\n",
    "Bash\n",
    "\n",
    "curl -X POST \"http://127.0.0.1:8000/summarize/\" \\\n",
    "     -H \"Content-Type: application/json\" \\\n",
    "     -d '{\"text\": \"The new marketing campaign will launch next month. It targets a younger demographic and will focus on social media channels to increase brand awareness.\"}'"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
