{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "251c522a",
   "metadata": {},
   "source": [
    "\n",
    "# 11.4.1 Enhancing AI‑Driven Customer Support  \n",
    "**Chapter 11: Coreference Resolution & Text Entailment**\n",
    "\n",
    "This notebook demonstrates a practical, end‑to‑end use case for **AI‑driven customer support** that aligns with Chapter 11’s focus on **coreference resolution** and **text entailment**.  \n",
    "We build a retrieval‑augmented question answering (QA) system over a small customer‑support knowledge base, and show:\n",
    "- **Coreference‑aware retrieval** (simple, spaCy‑based heuristic) to clarify pronouns in user queries (e.g., “When can I cancel it?” → replace *it* with the last detected entity).\n",
    "- **Text entailment checking** (NLI scoring) to validate that answers are supported by retrieved context.\n",
    "- **Hybrid QA** with both **SLM extractive QA** (DistilBERT) and **LLM generative QA** (Gemma via Hugging Face Inference API with a BitNet fallback and a local tiny fallback).\n",
    "\n",
    "> **What you’ll get:** a working, reproducible pipeline that loads sample docs, indexes with embeddings + Chroma, retrieves passages, performs coref‑aware querying, answers with either extractive or generative models, and scores answers with NLI entailment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d54834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Install dependencies (CPU friendly defaults) ---\n",
    "import sys, subprocess\n",
    "\n",
    "def pip_install(pkgs):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\"] + pkgs)\n",
    "\n",
    "# Core libs\n",
    "pip_install([\n",
    "    \"langchain==0.2.11\",\n",
    "    \"langchain-community==0.2.10\",\n",
    "    \"langchain-chroma==0.1.2\",\n",
    "    \"langchain-huggingface==0.0.3\",\n",
    "    \"chromadb==0.5.5\",\n",
    "    \"sentence-transformers==3.0.1\",\n",
    "    \"transformers==4.43.4\",\n",
    "    \"accelerate==0.34.2\",\n",
    "    \"torch\",                # will select a CPU wheel by default\n",
    "    \"spacy==3.7.4\",\n",
    "])\n",
    "\n",
    "# Download a lightweight English pipeline for spaCy (used in coref heuristic)\n",
    "subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "\n",
    "print(\"✅ Dependencies installed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f219e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Imports & basic setup ---\n",
    "import os, shutil, random, json, math\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoTokenizer, AutoModelForSequenceClassification\n",
    ")\n",
    "\n",
    "import spacy\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFaceEndpoint, HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Path for vector store persistence\n",
    "PERSIST_DIR = \"./chroma_customer_support\"\n",
    "\n",
    "# Hugging Face token for hosted LLMs (optional but recommended for Gemma/BitNet)\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\") or os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "print(\"HF_TOKEN found?\" , bool(HF_TOKEN))\n",
    "print(\"Using persist dir:\", PERSIST_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cbca30",
   "metadata": {},
   "source": [
    "\n",
    "## Sample Customer Support Knowledge Base\n",
    "To keep the demo self‑contained, we’ll synthesize a small but realistic knowledge base for a fictional SaaS platform:\n",
    "\n",
    "- **Accounts & Security:** password reset, multi‑factor auth, account recovery  \n",
    "- **Billing & Plans:** trial, refunds, invoices, upgrades/downgrades, VAT/GST  \n",
    "- **Support & SLA:** hours, severity levels, response times  \n",
    "- **API & Rate Limits:** keys, throttling, status codes  \n",
    "- **Compliance & Privacy:** data retention, GDPR/CCPA, data residency  \n",
    "- **Cancellations:** end‑of‑term policy, prorations\n",
    "\n",
    "These documents are kept short and friendly for quick iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2618ba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Create a small in-memory corpus of customer-support docs ---\n",
    "docs = [\n",
    "    {\n",
    "        \"id\": \"accounts_security\",\n",
    "        \"title\": \"Accounts & Security\",\n",
    "        \"text\": (\n",
    "            \"Password resets are available via the 'Forgot Password' link on the login page. \"\n",
    "            \"Users receive a one-time reset email that expires in 30 minutes. \"\n",
    "            \"We support multi-factor authentication via authenticator apps. \"\n",
    "            \"If you cannot access your email, contact support to verify identity and trigger recovery.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"billing_plans\",\n",
    "        \"title\": \"Billing & Plans\",\n",
    "        \"text\": (\n",
    "            \"We offer Free, Pro, and Enterprise plans. Trials last 14 days on Pro. \"\n",
    "            \"Refunds are available within 7 days of charge for monthly Pro, subject to fair use. \"\n",
    "            \"Invoices are emailed automatically and accessible from the Billing portal. \"\n",
    "            \"VAT/GST is calculated based on your billing address.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"support_sla\",\n",
    "        \"title\": \"Support & SLA\",\n",
    "        \"text\": (\n",
    "            \"Support is available 24/7 via email and chat for Pro and Enterprise. \"\n",
    "            \"SLA response times by severity: Sev-1 within 1 hour, Sev-2 within 4 hours, Sev-3 within 1 business day. \"\n",
    "            \"Free plan users can access our community forum and knowledge base.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"api_limits\",\n",
    "        \"title\": \"API & Rate Limits\",\n",
    "        \"text\": (\n",
    "            \"All API requests require an API key in the Authorization header. \"\n",
    "            \"The default rate limit is 100 requests per minute per key. \"\n",
    "            \"Bursting may be temporarily allowed; exceeding limits returns HTTP 429 with a Retry-After header.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"privacy_compliance\",\n",
    "        \"title\": \"Compliance & Privacy\",\n",
    "        \"text\": (\n",
    "            \"We comply with GDPR and CCPA. \"\n",
    "            \"Customer data is retained for 30 days after account cancellation unless retention is required by law. \"\n",
    "            \"Enterprise customers can request EU-only data residency.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"cancellation_policy\",\n",
    "        \"title\": \"Cancellation Policy\",\n",
    "        \"text\": (\n",
    "            \"You can cancel any time from the Billing portal. \"\n",
    "            \"For monthly plans, cancellation takes effect at the end of the current billing period; no partial refunds. \"\n",
    "            \"Annual plans are non-refundable after 30 days, except where required by local law.\"\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(docs)} documents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74581b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Chunk, embed, and index in Chroma ---\n",
    "# Clean up previous index if it exists\n",
    "if os.path.exists(PERSIST_DIR):\n",
    "    shutil.rmtree(PERSIST_DIR)\n",
    "\n",
    "# Split into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = []\n",
    "for d in docs:\n",
    "    for chunk in splitter.split_text(d[\"text\"]):\n",
    "        chunks.append({\"page_content\": chunk, \"metadata\": {\"source\": d[\"id\"], \"title\": d[\"title\"]}})\n",
    "\n",
    "print(\"Total chunks:\", len(chunks))\n",
    "\n",
    "# Embeddings (SLM): all-MiniLM-L6-v2 is small & strong for retrieval\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Vector store\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=[\n",
    "        __import__(\"langchain\").schema.Document(page_content=c[\"page_content\"], metadata=c[\"metadata\"])\n",
    "        for c in chunks\n",
    "    ],\n",
    "    embedding=embeddings,\n",
    "    persist_directory=PERSIST_DIR\n",
    ")\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 4})\n",
    "print(\"✅ Vector store ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97793fb",
   "metadata": {},
   "source": [
    "\n",
    "## Coreference Resolution (Heuristic) & Text Entailment (NLI)\n",
    "- **Coreference:** We implement a lightweight, **spaCy‑based heuristic**: find the most recent named entity and replace ambiguous pronouns in the question. This is not as strong as neural coreference models but is simple and fast.\n",
    "- **Entailment:** We score whether the retrieved **context entails the answer** using a compact NLI model. If entailment is low, we can flag the answer as potentially unsupported.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbcbeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Coreference heuristic with spaCy (pronoun → last entity) ---\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "_PRONOUNS = {\"it\",\"they\",\"them\",\"he\",\"she\",\"him\",\"her\",\"its\",\"their\",\"theirs\",\"his\",\"hers\",\"this\",\"that\"}\n",
    "\n",
    "def resolve_coref(question: str, chat_history: str = \"\") -> str:\n",
    "    '''Very lightweight heuristic:\n",
    "    - Look at chat_history + question.\n",
    "    - Take the last named entity (ORG/PRODUCT/PERSON/GPE).\n",
    "    - Replace ambiguous pronouns in the question with that entity.\n",
    "    '''\n",
    "    doc = nlp((chat_history + \" \" + question).strip())\n",
    "    entities = [ent.text for ent in doc.ents if ent.label_ in (\"ORG\",\"PRODUCT\",\"PERSON\",\"GPE\")]\n",
    "    target = entities[-1] if entities else None\n",
    "    if not target:\n",
    "        return question\n",
    "\n",
    "    q_doc = nlp(question)\n",
    "    tokens = []\n",
    "    for t in q_doc:\n",
    "        if t.lower_ in _PRONOUNS:\n",
    "            tokens.append(target)\n",
    "        else:\n",
    "            tokens.append(t.text)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# --- Entailment (NLI) scorer ---\n",
    "NLI_MODEL = \"cross-encoder/nli-deberta-base\"  # compact & accurate\n",
    "nli_tokenizer = AutoTokenizer.from_pretrained(NLI_MODEL)\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained(NLI_MODEL)\n",
    "\n",
    "def nli_entailment(premise: str, hypothesis: str) -> (str, float):\n",
    "    '''Returns (label, score) where label in {'ENTAILMENT','NEUTRAL','CONTRADICTION'}'''\n",
    "    inputs = nli_tokenizer(premise, hypothesis, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        logits = nli_model(**inputs).logits\n",
    "    probs = torch.softmax(logits, dim=-1)[0].tolist()\n",
    "    idx = int(torch.argmax(logits, dim=-1))\n",
    "    label = nli_model.config.id2label[idx]\n",
    "    score = float(probs[idx])\n",
    "    return label.upper(), score\n",
    "\n",
    "print(\"✅ Coref heuristic & NLI loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debdd69d",
   "metadata": {},
   "source": [
    "\n",
    "## Extractive QA (SLM: DistilBERT)\n",
    "We use a classic extractive QA pipeline: **`distilbert-base-uncased-distilled-squad`**.  \n",
    "Given the retrieved context, it extracts the best answer span.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31a91b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Extractive QA pipeline ---\n",
    "qa_extractive = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=\"distilbert-base-uncased-distilled-squad\",\n",
    "    tokenizer=\"distilbert-base-uncased-distilled-squad\",\n",
    ")\n",
    "\n",
    "def ask_question_extractive(question: str, chat_history: str = \"\", top_k:int = 4):\n",
    "    q_resolved = resolve_coref(question, chat_history=chat_history)\n",
    "    docs = retriever.get_relevant_documents(q_resolved)\n",
    "    if not docs:\n",
    "        return {\"answer\": \"\", \"context\": \"\", \"entailment\": (\"NEUTRAL\", 0.0), \"resolved_question\": q_resolved}\n",
    "    # Use the top document for extractive QA context\n",
    "    context = docs[0].page_content\n",
    "    result = qa_extractive(question=q_resolved, context=context)\n",
    "    label, score = nli_entailment(context, result[\"answer\"])\n",
    "    return {\n",
    "        \"answer\": result[\"answer\"],\n",
    "        \"score\": float(result.get(\"score\", 0.0)),\n",
    "        \"resolved_question\": q_resolved,\n",
    "        \"context_preview\": context[:400] + (\"...\" if len(context) > 400 else \"\"),\n",
    "        \"entailment\": (label, score),\n",
    "        \"sources\": [d.metadata for d in docs]\n",
    "    }\n",
    "\n",
    "print(\"✅ Extractive QA ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157b0855",
   "metadata": {},
   "source": [
    "\n",
    "## Generative QA (LLM)\n",
    "We prefer a hosted LLM via **Hugging Face Inference API** for convenience:\n",
    "- Primary: `google/gemma-7b-it`\n",
    "- Fallback: `microsoft/bitnet-b1.58-2B-4T` *(if available)*\n",
    "- Local tiny fallback (no token required): `distilgpt2` wrapped with `HuggingFacePipeline`\n",
    "\n",
    "> Set your token in the environment as `HF_TOKEN` or `HUGGINGFACEHUB_API_TOKEN` to use hosted models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c142aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Initialize LLM(s) ---\n",
    "llm = None\n",
    "llm_name = None\n",
    "\n",
    "def try_init_endpoint(repo_id: str):\n",
    "    global llm, llm_name\n",
    "    try:\n",
    "        _llm = HuggingFaceEndpoint(\n",
    "            repo_id=repo_id,\n",
    "            huggingface_api_token=HF_TOKEN,\n",
    "            task=\"text-generation\",\n",
    "            temperature=0.2,\n",
    "            max_new_tokens=256,\n",
    "            repetition_penalty=1.05,\n",
    "        )\n",
    "        # A small probe call to validate connectivity\n",
    "        _ = _llm.invoke(\"Say 'ready'.\")\n",
    "        llm = _llm\n",
    "        llm_name = repo_id\n",
    "        print(f\"✅ Using hosted LLM: {repo_id}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Could not init {repo_id} via endpoint -> {e}\")\n",
    "        return False\n",
    "\n",
    "if HF_TOKEN:\n",
    "    if not try_init_endpoint(\"google/gemma-7b-it\"):\n",
    "        # BitNet repo name may vary; this is best-effort. If it fails, we fall back locally.\n",
    "        try_init_endpoint(\"microsoft/bitnet-b1.58-2B-4T\")\n",
    "\n",
    "if llm is None:\n",
    "    # Local tiny fallback\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline as hf_pipeline\n",
    "    tok = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "    mdl = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "    gen_pipe = hf_pipeline(\"text-generation\", model=mdl, tokenizer=tok, max_new_tokens=256)\n",
    "    llm = HuggingFacePipeline(pipeline=gen_pipe)\n",
    "    llm_name = \"distilgpt2 (local fallback)\"\n",
    "    print(\"✅ Using local tiny fallback LLM: distilgpt2\")\n",
    "\n",
    "print(\"LLM initialized as:\", llm_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c29c15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Build a standard RetrievalQA chain (stuff prompt) ---\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "def format_context(docs) -> str:\n",
    "    blocks = []\n",
    "    for i, d in enumerate(docs, 1):\n",
    "        meta = d.metadata\n",
    "        blocks.append(f\"[{i}] {meta.get('title','')} ({meta.get('source','')})\\n{d.page_content}\")\n",
    "    return \"\\n\\n\".join(blocks)\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a helpful, precise customer-support assistant. \"\n",
    "    \"Answer ONLY from the provided context. If the answer is not present, say you don't know.\"\n",
    ")\n",
    "\n",
    "def ask_question_generative(question: str, chat_history: str = \"\"):\n",
    "    q_resolved = resolve_coref(question, chat_history=chat_history)\n",
    "    result = qa_chain({\"query\": f\"{SYSTEM_PROMPT}\\\\n\\\\nQuestion: {q_resolved}\"})\n",
    "    answer = result[\"result\"]\n",
    "    sources = result.get(\"source_documents\", []) or []\n",
    "    context_text = format_context(sources) if sources else \"\"\n",
    "    # Compute entailment between concatenated context and answer\n",
    "    label, score = nli_entailment(context_text, answer) if context_text else (\"NEUTRAL\", 0.0)\n",
    "    return {\n",
    "        \"answer\": answer.strip(),\n",
    "        \"resolved_question\": q_resolved,\n",
    "        \"entailment\": (label, score),\n",
    "        \"sources\": [s.metadata for s in sources],\n",
    "        \"context_preview\": (context_text[:600] + (\"...\" if len(context_text) > 600 else \"\")) if context_text else \"\"\n",
    "    }\n",
    "\n",
    "print(\"✅ Generative QA chain ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204aa151",
   "metadata": {},
   "source": [
    "\n",
    "## Try it out\n",
    "Ask a few common customer‑support questions. We’ll show both **extractive** and **generative** answers plus **entailment** scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f372a3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_questions = [\n",
    "    \"How do I reset my password?\",\n",
    "    \"What are the SLA response times for Sev-1 and Sev-2?\",\n",
    "    \"Can I get a refund on Pro?\",\n",
    "    \"What is the API rate limit and what happens if I exceed it?\",\n",
    "    \"Do you support EU-only data residency?\",\n",
    "    \"If I cancel it now, when does it take effect?\"\n",
    "]\n",
    "\n",
    "for q in test_questions:\n",
    "    print(\"=\"*90)\n",
    "    print(\"Q:\", q)\n",
    "    print(\"--- Extractive QA ---\")\n",
    "    ex = ask_question_extractive(q)\n",
    "    print(json.dumps(ex, indent=2))\n",
    "\n",
    "    print(\"\\n--- Generative QA ---\")\n",
    "    ge = ask_question_generative(q)\n",
    "    print(json.dumps(ge, indent=2))\n",
    "\n",
    "print(\"\\nTip: Set HF_TOKEN for better LLM results (Gemma via Inference API).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb869db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Ask your own question here ---\n",
    "user_q = \"When are refunds available for monthly Pro?\"\n",
    "print(\"Q:\", user_q)\n",
    "print(\"\\nExtractive:\", json.dumps(ask_question_extractive(user_q), indent=2))\n",
    "print(\"\\nGenerative:\", json.dumps(ask_question_generative(user_q), indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbed1f8e",
   "metadata": {},
   "source": [
    "\n",
    "## Deployment Options & Scalability\n",
    "- **Local / Internal:** Run this pipeline inside your private network. Persist the Chroma index and secure the LLM endpoints with org‑scoped tokens.\n",
    "- **Cloud:** Use managed vector DBs (e.g., a server‑hosted Chroma/PGVector) and LLM endpoints on Hugging Face or other providers. Horizontal scale the retriever API.\n",
    "- **Containerization (Docker):** Package the notebook code into a lightweight API service. Add observability for retrieval quality (hit rate, latency) and answer safety (entailment score thresholds).\n",
    "- **Chapter alignment:** The pipeline demonstrates **coreference‑aware retrieval** and **NLI‑based validation**, two core capabilities from Chapter 11.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68c7afa",
   "metadata": {},
   "source": [
    "\n",
    "### Troubleshooting\n",
    "- If the **hosted LLM** fails (no token or repo unavailable), the notebook automatically falls back to **`distilgpt2`**.  \n",
    "- If **PyTorch** installation fails on your platform, install it with your platform‑specific command from the official PyTorch site and rerun the install cell.\n",
    "- For **better coreference**, replace the heuristic with a neural coref model (e.g., via spaCy/AllenNLP) if your environment permits.\n",
    "- For **faster indexing**, switch to larger batch embeddings or a GPU‑enabled environment.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
