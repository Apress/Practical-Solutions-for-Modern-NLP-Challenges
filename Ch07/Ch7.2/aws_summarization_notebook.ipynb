{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9cb8099",
   "metadata": {},
   "source": [
    "# AWS Summarization Deployment\n",
    "This notebook includes both Lambda-based and SageMaker-based deployments for summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eced0bdf",
   "metadata": {},
   "source": [
    "## 1. Extractive Summarization on Lambda using DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3929cbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class Article(BaseModel):\n",
    "    text: str\n",
    "\n",
    "@app.post(\"/summarize/\")\n",
    "async def summarize(article: Article):\n",
    "    result = summarizer(article.text, max_length=120, min_length=30, do_sample=False)\n",
    "    return {\"summary\": result[0][\"summary_text\"]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4014e55",
   "metadata": {},
   "source": [
    "## 2. Abstractive Summarization with T5 on SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ce4817",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"xsum\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "def preprocess(example):\n",
    "    input_text = \"summarize: \" + example[\"document\"]\n",
    "    model_inputs = tokenizer(input_text, max_length=512, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(example[\"summary\"], max_length=128, truncation=True, padding=\"max_length\").input_ids\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "tokenized = dataset.map(preprocess, batched=True)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./t5-summarization\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    save_total_limit=1,\n",
    ")\n",
    "trainer = Trainer(model=model, args=args, train_dataset=tokenized[\"train\"])\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"t5-summarizer\")\n",
    "tokenizer.save_pretrained(\"t5-summarizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b1cc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_dir)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_dir)\n",
    "    return {\"model\": model, \"tokenizer\": tokenizer}\n",
    "\n",
    "def predict_fn(input_data, model_obj):\n",
    "    text = input_data[\"text\"]\n",
    "    tokenizer = model_obj[\"tokenizer\"]\n",
    "    model = model_obj[\"model\"]\n",
    "\n",
    "    input_ids = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", truncation=True)\n",
    "    summary_ids = model.generate(input_ids, max_length=150, num_beams=2)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return {\"summary\": summary}\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
