{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc4a06a8",
   "metadata": {},
   "source": [
    "\n",
    "# TinyBERT for Real-Time NER on Mobile Devices/Edge Computing\n",
    "\n",
    "## Introduction\n",
    "TinyBERT is a distilled, compact version of BERT, designed to achieve high accuracy with much smaller size and faster inference. \n",
    "In this notebook, we implement TinyBERT for Named Entity Recognition (NER), focusing on real-time inference on edge devices (like mobile phones or IoT devices).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331977ec",
   "metadata": {},
   "source": [
    "\n",
    "## Setup\n",
    "We will use the Hugging Face `transformers` library and `torch` for model inference. Optionally, optimization tools can be applied for enhanced performance.\n",
    "\n",
    "Install required packages:\n",
    "\n",
    "```bash\n",
    "!pip install transformers torch matplotlib\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97643455",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
    "from torch.quantization import quantize_dynamic\n",
    "from time import time\n",
    "from typing import List, Dict\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef22552",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_tinybert_model(model_name: str = \"adel-cybral/TinyBERT-finetuned-NER\"):\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "        logger.info(\"Model loaded successfully!\")\n",
    "        return model, tokenizer\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading model: {e}\")\n",
    "        return None, None\n",
    "\n",
    "model_name = \"adel-cybral/TinyBERT-finetuned-NER\"\n",
    "model, tokenizer = load_tinybert_model(model_name)\n",
    "if model is None or tokenizer is None:\n",
    "    raise Exception(\"Model or tokenizer could not be loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5a0109",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b43743",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "texts = [\n",
    "    \"John Doe works at Acme Corp in San Francisco.\",\n",
    "    \"Alice Johnson lives in New York City and works at Microsoft.\",\n",
    "    \"Google was founded by Larry Page and Sergey Brin while they were students at Stanford University.\",\n",
    "    \"Elon Musk is the CEO of SpaceX and Tesla.\",\n",
    "    \"Dr. Strange is a fictional character in the Marvel Universe.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bc784d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def perform_inference(pipeline, texts: List[str]) -> Dict[str, List[Dict]]:\n",
    "    inference_results = {}\n",
    "    for idx, text in enumerate(texts):\n",
    "        start_time = time()\n",
    "        entities = pipeline(text)\n",
    "        end_time = time()\n",
    "        inference_results[f\"Sentence_{idx+1}\"] = {\n",
    "            \"entities\": entities,\n",
    "            \"time_taken\": end_time - start_time\n",
    "        }\n",
    "    return inference_results\n",
    "\n",
    "inference_results = perform_inference(ner_pipeline, texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b87e99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for sentence, result in inference_results.items():\n",
    "    print(f\"{sentence} - Inference Time: {result['time_taken']:.4f} seconds\")\n",
    "    print(json.dumps(result['entities'], indent=2))\n",
    "    print(\"\n",
    "\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72594eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inference_times = [result['time_taken'] for result in inference_results.values()]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(texts)), inference_times, color='blue')\n",
    "plt.xlabel(\"Sentence Index\")\n",
    "plt.ylabel(\"Inference Time (seconds)\")\n",
    "plt.title(\"Inference Time per Sentence (Before Optimization)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfb5798",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def quantize_model(model):\n",
    "    try:\n",
    "        quantized_model = quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "        logger.info(\"Model quantization successful!\")\n",
    "        return quantized_model\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during model quantization: {e}\")\n",
    "        return None\n",
    "\n",
    "model_quantized = quantize_model(model)\n",
    "if model_quantized is None:\n",
    "    raise Exception(\"Quantization failed!\")\n",
    "\n",
    "ner_pipeline_quant = pipeline(\"ner\", model=model_quantized, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec0ce79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inference_results_quant = perform_inference(ner_pipeline_quant, texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e811a65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inference_times_quant = [result['time_taken'] for result in inference_results_quant.values()]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(len(texts)), inference_times, color='blue', alpha=0.6, label='Original Model')\n",
    "plt.bar(range(len(texts)), inference_times_quant, color='green', alpha=0.6, label='Quantized Model')\n",
    "plt.xlabel(\"Sentence Index\")\n",
    "plt.ylabel(\"Inference Time (seconds)\")\n",
    "plt.title(\"Comparison of Inference Time (Original vs Quantized Model)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d120db9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_results_to_file(results: Dict, filename: str = \"ner_results.json\"):\n",
    "    try:\n",
    "        with open(filename, \"w\") as file:\n",
    "            json.dump(results, file, indent=2)\n",
    "        logger.info(f\"Results saved to {filename}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving results: {e}\")\n",
    "\n",
    "save_results_to_file(inference_results)\n",
    "save_results_to_file(inference_results_quant, \"ner_results_quant.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c33dd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_performance(original_times: List[float], quantized_times: List[float]):\n",
    "    for i, (orig, quant) in enumerate(zip(original_times, quantized_times)):\n",
    "        improvement = ((orig - quant) / orig) * 100\n",
    "        print(f\"Sentence {i+1}: Original Time = {orig:.4f}s, Quantized Time = {quant:.4f}s, Improvement = {improvement:.2f}%\")\n",
    "\n",
    "compare_performance(inference_times, inference_times_quant)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
