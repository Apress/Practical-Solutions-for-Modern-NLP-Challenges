{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6c1c833",
   "metadata": {},
   "source": [
    "\n",
    "# 6.3.2 - Fine-Tuning T5 for Generative QA\n",
    "\n",
    "In this notebook, we'll demonstrate how to fine-tune the T5 model on a QA dataset. T5 (Text-to-Text Transfer Transformer) is a flexible model that reformulates all tasks into a text generation format.\n",
    "\n",
    "We will:\n",
    "- Load the SQuAD dataset using `datasets`\n",
    "- Preprocess the data for T5's format\n",
    "- Tokenize the inputs and targets\n",
    "- Fine-tune `t5-small` on QA using `Trainer`\n",
    "- Evaluate and generate sample predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc655590",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install transformers datasets accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063fd8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064bb763",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = load_dataset(\"squad\")\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860a271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(example):\n",
    "    input_text = f\"question: {example['question']} context: {example['context']}\"\n",
    "    target_text = example[\"answers\"][\"text\"][0] if example[\"answers\"][\"text\"] else \"\"\n",
    "    inputs = tokenizer(input_text, max_length=512, padding=\"max_length\", truncation=True)\n",
    "    targets = tokenizer(target_text, max_length=64, padding=\"max_length\", truncation=True)\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9ec3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenized_dataset = dataset.map(preprocess, batched=True)\n",
    "train_dataset = tokenized_dataset[\"train\"]\n",
    "val_dataset = tokenized_dataset[\"validation\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ea97db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./t5-qa\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35310e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db685315",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f801d01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.save_pretrained(\"./t5-finetuned-qa\")\n",
    "tokenizer.save_pretrained(\"./t5-finetuned-qa\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3967dc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_question = \"What is the capital of France?\"\n",
    "test_context = \"France is a country in Europe. The capital of France is Paris.\"\n",
    "\n",
    "input_text = f\"question: {test_question} context: {test_context}\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "output_ids = model.generate(input_ids, max_length=64)\n",
    "answer = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(\"Generated Answer:\", answer)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
