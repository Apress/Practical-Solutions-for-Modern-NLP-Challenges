{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff2dbfdc",
   "metadata": {},
   "source": [
    "\n",
    "# 6.3.7 - Evaluation Metrics for QA Performance\n",
    "\n",
    "In this notebook, we explore common evaluation metrics used to assess QA model performance, especially for both extractive and generative tasks.\n",
    "\n",
    "Metrics Covered:\n",
    "- Exact Match (EM)\n",
    "- F1 Score\n",
    "- Semantic F1\n",
    "- Latency & Throughput\n",
    "- ROUGE/BLEU for generative QA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31932834",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install evaluate transformers datasets sentence-transformers rouge-score pyRAPL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea19062",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from evaluate import load\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from rouge_score import rouge_scorer\n",
    "import time\n",
    "import pyRAPL\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96855210",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preds = [\"Paris\", \"Isaac Newton\", \"299,792,458 m/s\"]\n",
    "refs = [\"Paris\", \"Newton\", \"299792458 meters per second\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bcc1ec",
   "metadata": {},
   "source": [
    "## Exact Match (EM) and F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7455c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metric = load(\"squad\")\n",
    "results = metric.compute(predictions=preds, references=refs)\n",
    "print(f\"EM: {results['exact_match']:.2f}, F1: {results['f1']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ddd41e",
   "metadata": {},
   "source": [
    "## Semantic F1 with Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f17ac2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "semantic_scores = [util.cos_sim(model.encode(p), model.encode(r)).item() for p, r in zip(preds, refs)]\n",
    "semantic_f1 = sum(semantic_scores) / len(semantic_scores)\n",
    "print(f\"Semantic F1: {semantic_f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c692613",
   "metadata": {},
   "source": [
    "## ROUGE-L for Generative QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bacf6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "rouge_scores = [scorer.score(pred, ref)[\"rougeL\"].fmeasure for pred, ref in zip(preds, refs)]\n",
    "avg_rouge = sum(rouge_scores) / len(rouge_scores)\n",
    "print(f\"Average ROUGE-L: {avg_rouge:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b59c53",
   "metadata": {},
   "source": [
    "## Latency and Throughput Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f7033c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "question = \"Who discovered gravity?\"\n",
    "context = \"Isaac Newton formulated the law of universal gravitation.\"\n",
    "\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "\n",
    "start = time.time()\n",
    "_ = model(**inputs)\n",
    "end = time.time()\n",
    "\n",
    "latency = end - start\n",
    "throughput = 1 / latency\n",
    "\n",
    "print(f\"Latency: {latency:.4f}s, Throughput: {throughput:.2f} queries/sec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da622d1b",
   "metadata": {},
   "source": [
    "## Energy per Query (CPU Usage Estimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628399b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pyRAPL.setup()\n",
    "\n",
    "@pyRAPL.measureit()\n",
    "def run_query():\n",
    "    return model(**inputs)\n",
    "\n",
    "result = run_query()\n",
    "print(f\"Energy consumption: {result.energy} ÂµJ\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
