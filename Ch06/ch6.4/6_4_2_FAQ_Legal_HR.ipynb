{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFmLcqDv82Fh"
      },
      "outputs": [],
      "source": [
        "pip install langchain chromadb huggingface_hub sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3wc43hW83-g"
      },
      "outputs": [],
      "source": [
        "pip install -U langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rXFN1ZKAHzl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import os\n",
        "\n",
        "# Set your Hugging Face API token directly\n",
        "os.environ[\"HF_TOKEN\"] = \"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Data Input construction\n",
        "\n",
        "# 1. Create a folder named data in the same directory as your Python script.\n",
        "# 2. Save the below text above into a file named legal_faqs.txt inside the data folder.\n",
        "\n",
        "\n",
        "\n",
        "# What is a contract?\n",
        "# A contract is a legally binding agreement between two or more parties. It outlines the terms and conditions that the parties have agreed upon. For a contract to be valid, it generally needs to include an offer, acceptance of that offer, and consideration (something of value exchanged between the parties).\n",
        "\n",
        "# What is intellectual property?\n",
        "# Intellectual property (IP) refers to creations of the mind, such as inventions; literary and artistic works; designs; and symbols, names and images used in commerce. IP is protected in law by, for example, patents, copyright and trademarks, which enable people to earn recognition or financial benefit from what they invent or create.\n",
        "\n",
        "# What is negligence?\n",
        "# In legal terms, negligence is a failure to exercise the care that a reasonably prudent person would exercise in similar circumstances. This can result in harm or loss to another person. To prove negligence, one generally needs to show a duty of care existed, that this duty was breached, that the breach caused an injury, and that there were actual damages.\n",
        "\n",
        "# What is a tort?\n",
        "# A tort is a civil wrong that causes someone else to suffer loss or harm resulting in legal liability for the person who commits the tortious act. Common torts include negligence, defamation, and trespass. Unlike criminal law, which deals with wrongs against the state, tort law deals with wrongs against individuals.\n",
        "\n",
        "# What is the difference between a misdemeanor and a felony?\n",
        "# In the United States legal system, crimes are often classified as either misdemeanors or felonies. Generally, a felony is a more serious crime that can result in a sentence of more than one year in prison. Misdemeanors are less serious and typically carry penalties such as fines or jail time of less than a year. The specific definitions and classifications can vary by jurisdiction.\n",
        "\n"
      ],
      "metadata": {
        "id": "qtZ6OCIB1j6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6VNKeQJ74DZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import List\n",
        "from getpass import getpass\n",
        "\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_community.llms import HuggingFaceEndpoint\n",
        "from huggingface_hub import login\n",
        "\n",
        "# 1. Authentication with error handling\n",
        "def authenticate_hf():\n",
        "    \"\"\"Ensures valid Hugging Face credentials\"\"\"\n",
        "    try:\n",
        "        token = os.environ.get(\"HF_TOKEN\") or getpass(\"Enter HF token: \")\n",
        "        login(token=token, add_to_git_credential=False)\n",
        "        os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = token\n",
        "        return token\n",
        "    except Exception as e:\n",
        "        print(f\"Authentication failed: {str(e)}\")\n",
        "        exit(1)\n",
        "\n",
        "# 2. Document processing pipeline\n",
        "def initialize_rag_system():\n",
        "    \"\"\"Modern RAG initialization with proper chain setup\"\"\"\n",
        "    authenticate_hf()\n",
        "\n",
        "    # Document loading\n",
        "    documents = TextLoader(\"./data/legal_faqs.txt\").load()\n",
        "\n",
        "    # Text splitting\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=100,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \". \", \"? \", \"! \"]\n",
        "    )\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "    # Vector store\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"all-mpnet-base-v2\")\n",
        "    db = Chroma.from_documents(chunks, embeddings)\n",
        "    retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "    # LLM initialization\n",
        "    llm = HuggingFaceEndpoint(\n",
        "        repo_id=\"google/gemma-7b-it\",\n",
        "        temperature=0.1,\n",
        "        task=\"text-generation\",\n",
        "        max_new_tokens=300,\n",
        "    )\n",
        "\n",
        "    # Prompt template\n",
        "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "    Answer the question based only on the context:\n",
        "    Context: {context}\n",
        "    Question: {input}\n",
        "    Answer:\n",
        "    \"\"\")\n",
        "\n",
        "    # Create document chain\n",
        "    document_chain = create_stuff_documents_chain(llm, prompt)\n",
        "\n",
        "    # Create retrieval chain\n",
        "    return create_retrieval_chain(retriever, document_chain)\n",
        "\n",
        "# 3. Query handling\n",
        "def ask_question(qa_system, query: str):\n",
        "    \"\"\"Proper invocation with input formatting\"\"\"\n",
        "    try:\n",
        "        result = qa_system.invoke({\"input\": query})\n",
        "        print(f\"\\nQuestion: {query}\")\n",
        "        print(f\"Answer: {result['answer']}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Query failed: {str(e)}\")\n",
        "\n",
        "# 4. Main application flow\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Enterprise FAQ System - Secure RAG v3.0\")\n",
        "    try:\n",
        "        qa = initialize_rag_system()\n",
        "    except Exception as e:\n",
        "        print(f\"Initialization failed: {str(e)}\")\n",
        "        exit(1)\n",
        "\n",
        "    print(\"System ready. Type questions (exit to quit):\")\n",
        "    while True:\n",
        "        try:\n",
        "            user_input = input(\"\\n> \")\n",
        "            if user_input.lower() == \"exit\":\n",
        "                break\n",
        "            ask_question(qa, user_input)\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nExiting...\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwd-_TDVM_rx"
      },
      "outputs": [],
      "source": [
        "# BitNet introduces a fundamentally new approach to large language models (LLMs) by using extreme quantization-specifically, ternary weights (-1, 0, 1) encoded in just 1.58 bits per parameter. This innovation brings several notable benefits:\n",
        "\n",
        "# 1. Dramatic Efficiency Gains\n",
        "# Lower Memory Footprint: BitNet models require up to 7.2 times less memory than traditional 16-bit (FP16) models, enabling them to run efficiently on devices with limited resources, such as smartphones, laptops, and IoT devices.\n",
        "\n",
        "# Faster Inference: By replacing most floating-point multiplications with simple additions and subtractions, BitNet achieves up to 4.1 times faster inference and up to 8.9 times higher throughput, especially as model size increases.\n",
        "\n",
        "# Reduced Energy Consumption: BitNet slashes energy usage by 55â€“96% compared to full-precision LLMs, making it a sustainable choice for both edge devices and data centers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSixl7swM1T_"
      },
      "outputs": [],
      "source": [
        "# bitnett implmenetation\n",
        "\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import HuggingFaceHub\n",
        "\n",
        "def load_documents():\n",
        "    # Document loading implementation\n",
        "    return [Document(page_content=\"Your document content here\")]\n",
        "\n",
        "# Document processing pipeline\n",
        "documents = load_documents()\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "# Vector store configuration\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-mpnet-base-v2\")\n",
        "db = Chroma.from_documents(chunks, embeddings)\n",
        "\n",
        "# Model initialization with error handling\n",
        "try:\n",
        "    bitnet_llm = HuggingFaceHub(\n",
        "        repo_id=\"microsoft/bitnet-b1.58-2B-4T\",\n",
        "        model_kwargs={\"temperature\": 0.1, \"max_length\": 300}\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"BitNet initialization error: {e}\")\n",
        "    bitnet_llm = HuggingFaceHub(\n",
        "        repo_id=\"google/gemma-7b-it\",\n",
        "        model_kwargs={\"temperature\": 0.1, \"max_length\": 300}\n",
        "    )\n",
        "\n",
        "# QA system configuration\n",
        "qa_bitnet = RetrievalQA.from_chain_type(\n",
        "    llm=bitnet_llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db.as_retriever(search_kwargs={\"k\": 3})\n",
        ")\n",
        "\n",
        "def ask_question_bitnet(query):\n",
        "    \"\"\"Enhanced query handling with error protection\"\"\"\n",
        "    try:\n",
        "        response = qa_bitnet.run(query)\n",
        "        print(f\"[BitNet] Answer: {response}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Query processing error: {e}\")\n",
        "\n",
        "# Interactive interface\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Enhanced Legal/IT/HR FAQ System\")\n",
        "    while True:\n",
        "        user_input = input(\"> \")\n",
        "        if user_input.lower() == 'exit':\n",
        "            break\n",
        "        ask_question_bitnet(user_input)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To test the connection to hugging face model"
      ],
      "metadata": {
        "id": "ntynFK0qvVwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMgt9nt18EDp",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from getpass import getpass\n",
        "\n",
        "# Interactive authentication\n",
        "try:\n",
        "    login(token=os.environ[\"HF_TOKEN\"])\n",
        "except KeyError:\n",
        "    token = getpass(\"Enter Hugging Face token: \")\n",
        "    os.environ[\"HF_TOKEN\"] = token\n",
        "    login(token=token)\n",
        "\n",
        "# Updated LLM initialization\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id=\"google/gemma-7b-it\",\n",
        "    task=\"text-generation\",\n",
        "    model_kwargs={\n",
        "        \"temperature\": 0.1,\n",
        "        \"max_new_tokens\": 300,\n",
        "        \"return_full_text\": False\n",
        "    },\n",
        "    huggingfacehub_api_token=os.environ[\"HF_TOKEN\"]  # Explicit token\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "API_URL = \"https://api-inference.huggingface.co/models/google/gemma-7b-it\"\n",
        "headers = {\"Authorization\": f\"Bearer {os.environ['HF_TOKEN']}\"}\n",
        "response = requests.post(API_URL, headers=headers, json={\"inputs\": \"Hello\"})\n",
        "print(response.status_code)  # Should return 200"
      ],
      "metadata": {
        "id": "iIDVTvzmvbVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvustyocChrs"
      },
      "outputs": [],
      "source": [
        "from langchain_community.llms.huggingface_endpoint import HuggingFaceEndpoint\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    endpoint_url=\"google/gemma-7b-it\",  # Model ID only\n",
        "    max_new_tokens=300,\n",
        "    temperature=0.1,\n",
        "    huggingfacehub_api_token=os.environ[\"HF_TOKEN\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dhtm8shHBBq"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import whoami\n",
        "print(whoami())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dojpff9IJ1k"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "API_URL = \"https://api-inference.huggingface.co/models/google/gemma-7b-it\"\n",
        "headers = {\"Authorization\": f\"Bearer {os.environ['HF_TOKEN']}\"}\n",
        "response = requests.post(API_URL, headers=headers, json={\"inputs\": \"Hello\"})\n",
        "print(response.status_code)  # Should return 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_--iTRRQLJJA"
      },
      "outputs": [],
      "source": [
        "# deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A57llbMAMAts"
      },
      "outputs": [],
      "source": [
        "# Dockerfile\n",
        "# save the above code file as rag_faq.py\n",
        "FROM python:3.9-slim-buster\n",
        "\n",
        "WORKDIR /app\n",
        "\n",
        "COPY requirements.txt .\n",
        "RUN pip install -r requirements.txt\n",
        "\n",
        "COPY . .\n",
        "\n",
        "CMD [\"python\", \"rag_faq.py\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7eS70KzMIeQ"
      },
      "outputs": [],
      "source": [
        "#You would then build and run the container:\n",
        "docker build -t rag-faq .\n",
        "docker run -p 5000:5000 rag-faq # If you were to add a web interface\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhYI5rHcMSAK"
      },
      "outputs": [],
      "source": [
        "# 2. Cloud-Based Deployment:\n",
        "\n",
        "# Serverless Functions (e.g., AWS Lambda, Google Cloud Functions, Azure Functions): Package your RAG logic into a serverless function that gets triggered by user queries (e.g., via an API endpoint). This is cost-effective for applications with variable traffic. You would typically need to adapt your code to be event-driven.\n",
        "# Container Orchestration Services (e.g., Kubernetes on AWS EKS, Google GKE, Azure AKS): For more scalable and complex deployments, you can deploy your Dockerized application to a Kubernetes cluster.\n",
        "# Platform-as-a-Service (PaaS) (e.g., Heroku, Google App Engine, AWS Elastic Beanstalk): These platforms simplify deployment by managing the underlying infrastructure. You typically just need to provide your code and a configuration file.\n",
        "# Virtual Machines (VMs) (e.g., AWS EC2, Google Compute Engine, Azure Virtual Machines): You can provision a VM and run your application directly on it. This gives you more control over the environment but requires more management."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6DGkOPJMTWH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}