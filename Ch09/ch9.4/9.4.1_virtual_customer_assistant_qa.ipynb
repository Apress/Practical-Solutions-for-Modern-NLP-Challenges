{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a42cd53",
   "metadata": {},
   "source": [
    "# Virtual Customer Assistant: Question Answering over Internal Knowledge Bases\n",
    "\n",
    "**Chapter Alignment**: Chapter 9.4.1 of *Dialogue Systems* â€” this notebook demonstrates a hybrid SLM/LLM-based Virtual Customer Assistant that answers customer and employee queries by retrieving and synthesizing information from internal legal, HR, and IT knowledge sources.\n",
    "\n",
    "**Goals:**  \n",
    "- Provide fast extractive answers using a Small Language Model (SLM) (DistilBERT).  \n",
    "- Provide richer, generative answers using Large Language Models (LLMs) (e.g., Gemma and BitNet).  \n",
    "- Maintain a vectorized retrieval layer for context grounding via LangChain + Chroma.  \n",
    "- Offer a simple querying interface that can be embedded into a virtual assistant pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769c198f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Installation (run once) ======\n",
    "# You can uncomment and run these if not already installed in your environment\n",
    "# !pip install --upgrade pip\n",
    "# !pip install langchain transformers chromadb sentence-transformers accelerate torch gradio\n",
    "\n",
    "# ====== Environment setup ======\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Hugging Face API token (required for HuggingFaceHub/Gemma/BitNet access)\n",
    "# It is expected that the user sets HUGGINGFACEHUB_API_TOKEN in the environment or inputs it here.\n",
    "if \"HUGGINGFACEHUB_API_TOKEN\" not in os.environ:\n",
    "    print('HUGGINGFACEHUB_API_TOKEN not found in environment. Prompting for token (won\\'t be stored persistently).')\n",
    "    token = getpass('Enter your HuggingFace Hub API token: ')\n",
    "    os.environ['HUGGINGFACEHUB_API_TOKEN'] = token\n",
    "\n",
    "# Basic version info (for reproducibility)\n",
    "import sys\n",
    "print('Python version:', sys.version.split()[0])\n",
    "try:\n",
    "    import langchain, transformers, chromadb, torch, sentence_transformers\n",
    "    print('langchain version:', langchain.__version__)\n",
    "    import transformers as _transformers\n",
    "    print('transformers version:', _transformers.__version__)\n",
    "except ImportError as e:\n",
    "    print('Some libraries are not installed. Please install requirements as shown above.', str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48e4239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Sample Knowledge Documents ======\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Simulated internal knowledge base: legal, HR, and IT policies\n",
    "legal_doc = \"\"\"**Legal Knowledge Base**\\n\n",
    "1. Confidentiality agreements must be signed before sharing client data.\\n\n",
    "2. All contracts over $50,000 require a secondary legal review.\\n\n",
    "3. GDPR compliance demands a data access audit every 6 months.\\n\"\"\"\n",
    "\n",
    "hr_doc = \"\"\"**HR Knowledge Base**\\n\n",
    "1. Employees are entitled to 25 paid leave days per calendar year.\\n\n",
    "2. The onboarding process includes compliance training, software access setup, and orientation.\\n\n",
    "3. Performance reviews are conducted bi-annually.\\n\"\"\"\n",
    "\n",
    "it_doc = \"\"\"**IT Knowledge Base**\\n\n",
    "1. Passwords must be rotated every 90 days.\\n\n",
    "2. Multi-factor authentication (MFA) is required for VPN access.\\n\n",
    "3. Report any suspicious emails to the security team immediately.\\n\"\"\"\n",
    "\n",
    "documents = [\n",
    "    Document(page_content=legal_doc, metadata={'source': 'legal'}),\n",
    "    Document(page_content=hr_doc, metadata={'source': 'hr'}),\n",
    "    Document(page_content=it_doc, metadata={'source': 'it'}),\n",
    "]\n",
    "\n",
    "print('Created sample documents for Legal, HR, and IT knowledge bases.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03955932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Chunking and Vector Store Construction ======\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Split documents into chunks to serve as retrievable context\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f'Number of document chunks: {len(docs)}')\n",
    "\n",
    "# Embeddings - use a relatively small embedding model for demo (semantic search)\n",
    "embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "\n",
    "# Build Chroma vector store (in-memory)\n",
    "vector_store = Chroma.from_documents(docs, embeddings, collection_name=\"virtual_customer_assistant\")\n",
    "\n",
    "# Retriever for downstream QA\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={'k': 3})\n",
    "print('Vector store and retriever initialized.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f1f5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Extractive QA with a Small Language Model (SLM) - DistilBERT ======\n",
    "from transformers import pipeline\n",
    "\n",
    "# Use a pre-trained DistilBERT model fine-tuned on SQuAD (extractive QA)\n",
    "extractive_pipeline = pipeline('question-answering', model='distilbert-base-uncased-distilled-squad', tokenizer='distilbert-base-uncased-distilled-squad')\n",
    "\n",
    "def ask_extractive(question: str):\n",
    "    # Retrieve top contexts\n",
    "    retrieved_docs = retriever.get_relevant_documents(question)\n",
    "    combined_context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "    # Query the extractive QA pipeline with concatenated context\n",
    "    answer = extractive_pipeline(question=question, context=combined_context)\n",
    "    return {\n",
    "        'answer': answer.get('answer'),\n",
    "        'score': answer.get('score'),\n",
    "        'source_chunks': [doc.metadata for doc in retrieved_docs],\n",
    "        'context': combined_context\n",
    "    }\n",
    "\n",
    "# Demo extractive QA\n",
    "q1 = \"What is the leave policy for employees?\"\n",
    "res1 = ask_extractive(q1)\n",
    "print('Question:', q1)\n",
    "print('Extractive Answer:', res1['answer'])\n",
    "print('Score:', res1['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4c317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Generative QA with LLMs (Gemma & BitNet) ======\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Initialize Gemma (LLM generative model) via HuggingFaceHub\n",
    "try:\n",
    "    gemma = HuggingFaceHub(repo_id='google/gemma-7b-it', model_kwargs={'temperature':0.2, 'max_length':256})\n",
    "    qa_gemma = RetrievalQA.from_chain_type(llm=gemma, chain_type='stuff', retriever=retriever)\n",
    "    print('Gemma-based generative QA chain initialized.')\n",
    "except Exception as e:\n",
    "    print('Failed to initialize Gemma LLM, falling back to flan-t5-small. Error:', str(e))\n",
    "    from langchain.llms import HuggingFaceHub as _HFH\n",
    "    gemma = _HFH(repo_id='google/flan-t5-small', model_kwargs={'temperature':0.2, 'max_length':256})\n",
    "    qa_gemma = RetrievalQA.from_chain_type(llm=gemma, chain_type='stuff', retriever=retriever)\n",
    "\n",
    "# Initialize BitNet LLM via HuggingFaceHub\n",
    "try:\n",
    "    bitnet = HuggingFaceHub(repo_id='microsoft/bitnet-b1.58-2B-4T', model_kwargs={'temperature':0.2, 'max_length':256})\n",
    "    qa_bitnet = RetrievalQA.from_chain_type(llm=bitnet, chain_type='stuff', retriever=retriever)\n",
    "    print('BitNet-based generative QA chain initialized.')\n",
    "except Exception as e:\n",
    "    print('Failed to initialize BitNet LLM, falling back to flan-t5-small. Error:', str(e))\n",
    "    bitnet = HuggingFaceHub(repo_id='google/flan-t5-small', model_kwargs={'temperature':0.2, 'max_length':256})\n",
    "    qa_bitnet = RetrievalQA.from_chain_type(llm=bitnet, chain_type='stuff', retriever=retriever)\n",
    "\n",
    "# Functions to ask questions\n",
    "def ask_question_gemma(question: str):\n",
    "    result = qa_gemma.run(question)\n",
    "    return result\n",
    "\n",
    "def ask_question_bitnet(question: str):\n",
    "    result = qa_bitnet.run(question)\n",
    "    return result\n",
    "\n",
    "# Demo generative QA\n",
    "q2 = \"What are the requirements for GDPR compliance?\"\n",
    "print('Question:', q2)\n",
    "print('Gemma Answer:', ask_question_gemma(q2))\n",
    "print('BitNet Answer:', ask_question_bitnet(q2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f47b622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Simple Virtual Customer Assistant Interface ======\n",
    "def virtual_assistant_loop():\n",
    "    print('Starting Virtual Customer Assistant. Type \"exit\" to quit.')\n",
    "    while True:\n",
    "        user_q = input('User: ').strip()\n",
    "        if user_q.lower() in ('exit', 'quit'):\n",
    "            print('Assistant: Goodbye!')\n",
    "            break\n",
    "        # First try extractive for concise/fast answer\n",
    "        extractive = ask_extractive(user_q)\n",
    "        print('\\n[SLM Extractive Answer]')\n",
    "        print(f\"Answer: {extractive['answer']} (score: {extractive['score']:.3f})\")\n",
    "        # Then generative augmentation\n",
    "        print('\\n[LLM Generative Answer - Gemma]')\n",
    "        try:\n",
    "            print(ask_question_gemma(user_q))\n",
    "        except Exception as e:\n",
    "            print('Gemma error:', str(e))\n",
    "        print('\\n[LLM Generative Answer - BitNet]')\n",
    "        try:\n",
    "            print(ask_question_bitnet(user_q))\n",
    "        except Exception as e:\n",
    "            print('BitNet error:', str(e))\n",
    "\n",
    "# Note: To run interactive mode, uncomment the next line\n",
    "# virtual_assistant_loop()\n",
    "print('Virtual assistant interface defined. You can call virtual_assistant_loop().')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eaef5a",
   "metadata": {},
   "source": [
    "## Deployment Options and Scalability\n",
    "\n",
    "This prototype can be extended and deployed in several ways:\n",
    "\n",
    "1. **Local Deployment:** Run the notebook code as a backend service (e.g., Flask/FastAPI) within a private corporate network for internal virtual customer assistants. Embed `ask_extractive` and `ask_question_gemma` into REST endpoints.  \n",
    "2. **Cloud Deployment:** Host on platforms like AWS (SageMaker endpoints for fine-tuned models, or serverless containers for deployment). Use Amazon Lex in front to handle intent recognition and route user queries to this QA backend.  \n",
    "3. **Containerization:** Package the system as a Docker container to ensure consistent environment across development, staging, and production.  \n",
    "4. **Hybrid SLM/LLM Strategy:** Use the extractive SLM (DistilBERT) for low-latency quick answers; fallback to LLMs only when the query requires synthesis, explanation, or when confidence from SLM is low.  \n",
    "5. **Conversation State & Context:** Extend the interface to maintain multi-turn context, injecting previous user turns into retrieval or prompt engineering for more coherent dialogues.  \n",
    "6. **Monitoring & Feedback Loop:** Log user questions and model answers; collect feedback to re-rank or fine-tune models periodically to improve accuracy in the domain-specific knowledge base.\n",
    "\n",
    "**Next Steps:**  \n",
    "- Add authentication and access control for sensitive knowledge.  \n",
    "- Plug into chat/voice channels (e.g., Slack bots, web chat widgets, Amazon Connect with Lex).  \n",
    "- Fine-tune the extractive model on internal corporate Q&A pairs for domain adaptation.  \n",
    "- Implement answer validation with guardrails (e.g., detect conflicting policy answers)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a17976",
   "metadata": {},
   "source": [
    "## Summary for Virtual Customer Assistant Use Case\n",
    "\n",
    "This notebook provides a working skeleton of a Virtual Customer Assistant that answers internal queries over legal, HR, and IT knowledge. It combines:\n",
    "\n",
    "- **Extractive QA (SLM):** DistilBERT gives fast, grounded spans from retrieved documents.  \n",
    "- **Generative QA (LLMs):** Gemma and BitNet synthesize more natural, explanatory responses while being grounded via retrieval.  \n",
    "- **Vector Retrieval Layer:** LangChain + Chroma handle semantic search across chunked knowledge documents.  \n",
    "- **Simple Interface:** A loop is provided to simulate conversation, enabling a multi-modal assistant to be built on top.\n",
    "\n",
    "The architecture is suitable for embedding into larger dialogue systems (e.g., with intent understanding via Lex or Rasa), and can be scaled across local, cloud, or hybrid deployments as described above."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
