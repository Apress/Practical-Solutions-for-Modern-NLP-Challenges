{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Fine-Tuning SLMs & LLMs on SageMaker**\n",
    "\n",
    "Fine-tuning helps adapt pre-trained models (like BERT, RoBERTa, DistilBERT, TinyBERT) to specific tasks such as sentiment analysis, topic classification, or entity recognition.\n",
    "\n",
    "### **1. Process Overview**\n",
    "1. **Choose a Pre-trained Model**  \n",
    "   - Select an SLM (e.g., DistilBERT, TinyBERT) or LLM (e.g., BERT, RoBERTa) from **Hugging Face Model Hub** or **SageMaker JumpStart**.\n",
    "   \n",
    "2. **Prepare Data**  \n",
    "   - Format data in **CSV, JSON, or Parquet**.\n",
    "   - Tokenize using `transformers` library.\n",
    "\n",
    "3. **Set Up SageMaker Environment**  \n",
    "   - Define an **Amazon SageMaker Training Job**.\n",
    "   - Use SageMakerâ€™s **Hugging Face Estimator**.\n",
    "\n",
    "4. **Fine-Tune the Model**  \n",
    "   - Define hyperparameters.\n",
    "   - Use distributed training if necessary.\n",
    "\n",
    "5. **Evaluate & Deploy**  \n",
    "   - Deploy using **SageMaker Endpoint**.\n",
    "   - Evaluate on test data.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Code Snippet for Fine-Tuning SLM/LLM on SageMaker**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Initialize SageMaker session & role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "# Define the Hugging Face model (BERT, RoBERTa, DistilBERT)\n",
    "hyperparameters = {\n",
    "    \"epochs\": 3,\n",
    "    \"train_batch_size\": 16,\n",
    "    \"model_name\": \"distilbert-base-uncased\",  # Change for LLMs like BERT, RoBERTa\n",
    "}\n",
    "\n",
    "# Define training script location\n",
    "train_script = \"train.py\"  # Custom training script\n",
    "\n",
    "# Set up SageMaker Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point=train_script,\n",
    "    source_dir=\"./\",\n",
    "    instance_type=\"ml.p3.2xlarge\",  # Use ml.g5 for large models\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    transformers_version=\"4.17\",\n",
    "    pytorch_version=\"1.10\",\n",
    "    py_version=\"py38\",\n",
    "    hyperparameters=hyperparameters,\n",
    ")\n",
    "\n",
    "# Start training job\n",
    "huggingface_estimator.fit({\"train\": \"s3://your-bucket/train-data/\"})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Deployment & Inference**\n",
    "Once training is complete, we can deploy the model as an API using a **SageMaker endpoint**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Deploy trained model\n",
    "predictor = huggingface_estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.large\"  # Change for large models\n",
    ")\n",
    "\n",
    "# Run inference\n",
    "input_text = {\"inputs\": \"What is the sentiment of this sentence?\"}\n",
    "response = predictor.predict(input_text)\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
